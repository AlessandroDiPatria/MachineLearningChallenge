{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2021/2022 - Challenge \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "yourNameSurname='Alessandro Di Patria' # e.g., yourNameSurname='Mario Rossi'\n",
    "yourMatricolaNumber='1844538' # e.g., yourMatricolaNumber='12345678'\n",
    "yourStudentEMAIL='dipatria.1844538@studenti.uniroma1.it' # e.g., yourStudentEMAIL='rossim.12345678@studenti.uniroma1.it'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1. Mandatory Rules:\n",
    "- This year the results of the challenges will count 11,2/28 of your final grade (full info about grades <a href='https://twiki.di.uniroma1.it/twiki/view/ApprAuto '>here</a>).\n",
    "- Only one submission is allowed. We will not consider multiple submissions.\n",
    "- Please remember your solution must be <b>\"YOUR SOLUTION\"</b>, hence you are requested to deliver your individual answers/arguments/opinions/critics.\n",
    "- Mail your solution (with your <b>jupyter notebook</b> and the cleaned dataset) only to stefano.faralli@uniroma1.it <b>deadlines are announced on the ML google group and <a href='https://twiki.di.uniroma1.it/twiki/view/ApprAuto'>here</a> (NO EXCEPTIONS)</b> if you miss to deliver your solution you must wait the next (if any) available deadline. \n",
    "- The subject of your email must be: \"[ML-21-22-Challenge_solution] NAME - SURNAME - MATRICOLA\".\n",
    "- Double check the subject of your email and the attachments.\n",
    "- In case you want to compress the attachment, <b>USE ONLY STANDARD ZIP compression</b> (NO RAR,7Z etc..).\n",
    "- <b>Please sumbit The notebook (with SAVED OUTPUTS) and the cleaned dataset!</b>.\n",
    "- Your solution might be considered as the \"copy\" of others solutions, in that specific case the resulting score for all involved students will be 0/8.\n",
    "- Then read carefully all the part of the jupyter notebook and fill all fields.\n",
    "- <b>solutions (and correspondig points) are evaluated mainly on your thoughts/comments/opinions</b>.  \n",
    "- If you have questions <b>Don't write \"personal\" emails</b> to Stefano Faralli, instead <b>use our google group</b>.\n",
    "- A solution having a summary discussion with less than 500 words is evaluated with 0 points.\n",
    "- Comments summary etc.. must be in <b>English</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "### Dataset and task Description:\n",
    "<img width='400' src='news-online.jpeg'/>\n",
    "\n",
    "- The challenge is about online news popularity;\n",
    "- The provided dataset consists of one single csv file (\"OnlineNewsPopularity.csv\");\n",
    "- The provided dataset is a modified <b>noisy</b> version of the original dataset described in [1];\n",
    "\n",
    "[1] K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision\n",
    "    Support System for Predicting the Popularity of Online News. Proceedings\n",
    "    of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence,\n",
    "    September, Coimbra, Portugal\n",
    "\n",
    "\n",
    "This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal of the task is to predict the number of shares in social networks (popularity).\n",
    "\n",
    "Number of Instances: <b>39,797</b> \n",
    "\n",
    "Number of Attributes: <b>61</b>\n",
    "\n",
    "Target: <b>shares</b>\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "<table>\n",
    " <tr><th> index </th><th>name</th><th>description</th></tr>\n",
    " <tr><td>0</td><td>url</td><td>URL of the article</td></tr>\n",
    " <tr><td>1</td><td>timedelta</td><td>Days between the article publication and the dataset acquisition</td></tr>\n",
    " <tr><td>2</td><td>n_tokens_title</td><td>Number of words in the title</td></tr>\n",
    " <tr><td>3</td><td>n_tokens_content</td><td>Number of words in the content</td></tr>\n",
    " <tr><td>4</td><td>n_unique_tokens</td><td>Rate of unique words in the content</td></tr>\n",
    " <tr><td>5</td><td>n_non_stop_words</td><td>Rate of non-stop words in the content</td></tr>\n",
    " <tr><td>6</td><td>n_non_stop_unique_tokens</td><td>Rate of unique non-stop words in content</td></tr>\n",
    " <tr><td>7</td><td>num_hrefs</td><td>Number of links</td></tr>\n",
    " <tr><td>8</td><td>num_self_hrefs</td><td>Number of links to other articles published by Mashable</td></tr>\n",
    " <tr><td>9</td><td>num_imgs</td><td>Number of images</td></tr>\n",
    " <tr><td>10</td><td>num_videos</td><td>Number of videos</td></tr>\n",
    " <tr><td>11</td><td>average_token_length</td><td>Average length of the words in the content</td></tr>\n",
    " <tr><td>12</td><td>num_keywords</td><td>Number of keywords in the metadata</td></tr>\n",
    " <tr><td>13</td><td>data_channel_is_lifestyle</td><td>Is data channel 'Lifestyle'?</td></tr>\n",
    " <tr><td>14</td><td>data_channel_is_entertainment</td><td>Is data channel 'Entertainment'?</td></tr>\n",
    " <tr><td>15</td><td>data_channel_is_bus</td><td>Is data channel 'Business'?</td></tr>\n",
    " <tr><td>16</td><td>data_channel_is_socmed</td><td>Is data channel 'Social Media'?</td></tr>\n",
    " <tr><td>17</td><td>data_channel_is_tech</td><td>Is data channel 'Tech'?</td></tr>\n",
    " <tr><td>18</td><td>data_channel_is_world</td><td>Is data channel 'World'?</td></tr>\n",
    " <tr><td>19</td><td>kw_min_min</td><td>Worst keyword (min. shares)</td></tr>\n",
    " <tr><td>20</td><td>kw_max_min</td><td>Worst keyword (max. shares)</td></tr>\n",
    " <tr><td>21</td><td>kw_avg_min</td><td>Worst keyword (avg. shares)</td></tr>\n",
    " <tr><td>22</td><td>kw_min_max</td><td>Best keyword (min. shares)</td></tr>\n",
    " <tr><td>23</td><td>kw_max_max</td><td>Best keyword (max. shares)</td></tr>\n",
    " <tr><td>24</td><td>kw_avg_max</td><td>Best keyword (avg. shares)</td></tr>\n",
    " <tr><td>25</td><td>kw_min_avg</td><td>Avg. keyword (min. shares)</td></tr>\n",
    " <tr><td>26</td><td>kw_max_avg</td><td>Avg. keyword (max. shares)</td></tr>\n",
    " <tr><td>27</td><td>kw_avg_avg</td><td>Avg. keyword (avg. shares)</td></tr>\n",
    " <tr><td>28</td><td>self_reference_min_shares</td><td>Min. shares of referenced articles in Mashable</td></tr>\n",
    " <tr><td>29</td><td>self_reference_max_shares</td><td>Max. shares of referenced articles in Mashable</td></tr>\n",
    " <tr><td>30</td><td>self_reference_avg_sharess</td><td>Avg. shares of referenced articles in Mashable</td></tr>\n",
    " <tr><td>31</td><td>weekday_is_monday</td><td>Was the article published on a Monday?</td></tr>\n",
    " <tr><td>32</td><td>weekday_is_tuesday</td><td>Was the article published on a Tuesday?</td></tr>\n",
    " <tr><td>33</td><td>weekday_is_wednesday</td><td>Was the article published on a Wednesday?</td></tr>\n",
    " <tr><td>34</td><td>weekday_is_thursday</td><td>Was the article published on a Thursday?</td></tr>\n",
    " <tr><td>35</td><td>weekday_is_friday</td><td>Was the article published on a Friday?</td></tr>\n",
    " <tr><td>36</td><td>weekday_is_saturday</td><td>Was the article published on a Saturday?</td></tr>\n",
    " <tr><td>37</td><td>weekday_is_sunday</td><td> Was the article published on a Sunday?</td></tr>\n",
    " <tr><td>38</td><td>is_weekend</td><td>Was the article published on the weekend?</td></tr>\n",
    " <tr><td>39</td><td>LDA_00</td><td>Closeness to LDA topic 0</td></tr>\n",
    " <tr><td>40</td><td>LDA_01</td><td>Closeness to LDA topic 1</td></tr>\n",
    " <tr><td>41</td><td>LDA_02</td><td>Closeness to LDA topic 2</td></tr>\n",
    " <tr><td>42</td><td>LDA_03</td><td>Closeness to LDA topic 3</td></tr>\n",
    " <tr><td>43</td><td>LDA_04</td><td>Closeness to LDA topic 4</td></tr>\n",
    " <tr><td>44</td><td>global_subjectivity</td><td>Text subjectivity</td></tr>\n",
    " <tr><td>45</td><td>global_sentiment_polarity</td><td>Text sentiment polarity</td></tr>\n",
    " <tr><td>46</td><td>global_rate_positive_words</td><td>Rate of positive words in the content</td></tr>\n",
    " <tr><td>47</td><td>global_rate_negative_words</td><td> Rate of negative words in the content</td></tr>\n",
    " <tr><td>48</td><td>rate_positive_words</td><td>Rate of positive words among non-neutral tokens</td></tr>\n",
    " <tr><td>49</td><td>rate_negative_words</td><td>Rate of negative words among non-neutral tokens</td></tr>\n",
    " <tr><td>50</td><td>avg_positive_polarity</td><td>Avg. polarity of positive words</td></tr>\n",
    " <tr><td>51</td><td>min_positive_polarity</td><td>Min. polarity of positive words</td></tr>\n",
    " <tr><td>52</td><td>max_positive_polarity</td><td>Max. polarity of positive words</td></tr>\n",
    " <tr><td>53</td><td>avg_negative_polarity</td><td>Avg. polarity of negative words</td></tr>\n",
    " <tr><td>54</td><td>min_negative_polarity</td><td>Min. polarity of negative words</td></tr>\n",
    " <tr><td>55</td><td>max_negative_polarity</td><td>Max. polarity of negative words</td></tr>\n",
    " <tr><td>56</td><td>title_subjectivity</td><td>Title subjectivity</td></tr>\n",
    " <tr><td>57</td><td>title_sentiment_polarity</td><td>Title polarity</td></tr>\n",
    " <tr><td>58</td><td>abs_title_subjectivity</td><td>Absolute subjectivity level</td></tr>\n",
    " <tr><td>59</td><td>abs_title_sentiment_polarity</td><td>Absolute polarity level</td></tr>\n",
    "     <tr><td>60</td><td>shares</td><td>Number of shares (target)</td></tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# 2. Pre-processing (up to 3 of 11.2 points)     \n",
    "     \n",
    "     \n",
    "## 2.1 Clean and Load the Dataset (up to 1 of 11.2 points)\n",
    "Use the following two cells (a code cell and, a markdown cell) to: \n",
    "- create a pandas DataFrame by loading a cleaned version of the \"OnlineNewsPopularity.cvs\" file.  \n",
    "- describe the identified noise and the methodology used to fix the problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-125.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.000000</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.000000</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39644</th>\n",
       "      <td>10.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>0.516355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.644128</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.076923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.356439</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39645</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>682.000000</td>\n",
       "      <td>0.539493</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.692661</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.205246</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39646</th>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>0.701987</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39647</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.471338</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39648 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "0          731.0            12.0        219.000000         0.663594   \n",
       "1          731.0             9.0        255.000000         0.604743   \n",
       "2          731.0             9.0        211.000000         0.575130   \n",
       "3          731.0             9.0        531.000000         0.503788   \n",
       "4          731.0            13.0       1072.000000         0.415646   \n",
       "...          ...             ...               ...              ...   \n",
       "39643        NaN             NaN               NaN              NaN   \n",
       "39644       10.0           442.0          0.516355         1.000000   \n",
       "39645        8.0             6.0        682.000000         0.539493   \n",
       "39646        8.0            10.0        157.000000         0.701987   \n",
       "39647        1.0             0.0          2.000000         4.471338   \n",
       "\n",
       "       n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  \\\n",
       "0              1.000000                  0.815385        4.0             2.0   \n",
       "1              1.000000                  0.791946        3.0             1.0   \n",
       "2              1.000000                  0.663866        3.0             1.0   \n",
       "3              1.000000                  0.665635        9.0             0.0   \n",
       "4              1.000000                  0.540890       19.0            19.0   \n",
       "...                 ...                       ...        ...             ...   \n",
       "39643               NaN                       NaN        NaN             NaN   \n",
       "39644          0.644128                 24.000000        1.0            12.0   \n",
       "39645          1.000000                  0.692661       10.0             1.0   \n",
       "39646          1.000000                  0.846154        NaN             NaN   \n",
       "39647          4.000000                  0.000000        1.0             0.0   \n",
       "\n",
       "       num_imgs  num_videos  ...  min_positive_polarity  \\\n",
       "0           1.0    0.000000  ...               0.100000   \n",
       "1           1.0    0.000000  ...               0.033333   \n",
       "2           1.0    0.000000  ...               0.100000   \n",
       "3           1.0    0.000000  ...               0.136364   \n",
       "4          20.0    0.000000  ...               0.033333   \n",
       "...         ...         ...  ...                    ...   \n",
       "39643       NaN         NaN  ...                    NaN   \n",
       "39644       1.0    5.076923  ...               0.500000   \n",
       "39645       1.0    0.000000  ...               0.062500   \n",
       "39646       NaN         NaN  ...                    NaN   \n",
       "39647       0.0    0.000000  ...               0.166667   \n",
       "\n",
       "       max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
       "0                   0.700000              -0.350000              -0.600000   \n",
       "1                   0.700000              -0.118750            -125.000000   \n",
       "2                   1.000000              -0.466667              -0.800000   \n",
       "3                   0.800000              -0.369697              -0.600000   \n",
       "4                   1.000000              -0.220192              -0.500000   \n",
       "...                      ...                    ...                    ...   \n",
       "39643                    NaN                    NaN                    NaN   \n",
       "39644              -0.356439              -0.800000              -0.166667   \n",
       "39645               0.500000              -0.205246              -0.500000   \n",
       "39646                    NaN                    NaN                    NaN   \n",
       "39647               0.250000            1300.000000                    NaN   \n",
       "\n",
       "       max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
       "0                  -0.200000            0.500000                 -0.187500   \n",
       "1                  -0.100000            0.000000                  0.000000   \n",
       "2                  -0.133333            0.000000                  0.000000   \n",
       "3                  -0.166667            0.000000                  0.000000   \n",
       "4                  -0.050000            0.454545                  0.136364   \n",
       "...                      ...                 ...                       ...   \n",
       "39643                    NaN                 NaN                       NaN   \n",
       "39644               0.454545            0.136364                  0.045455   \n",
       "39645              -0.012500            0.000000                  0.000000   \n",
       "39646                    NaN                 NaN                       NaN   \n",
       "39647                    NaN                 NaN                       NaN   \n",
       "\n",
       "       abs_title_subjectivity  abs_title_sentiment_polarity  shares  \n",
       "0                    0.000000                      0.187500   593.0  \n",
       "1                    0.500000                      0.000000   711.0  \n",
       "2                    0.500000                      0.000000  1500.0  \n",
       "3                    0.500000                      0.000000  1200.0  \n",
       "4                    0.045455                      0.136364   505.0  \n",
       "...                       ...                           ...     ...  \n",
       "39643                     NaN                           NaN     NaN  \n",
       "39644                0.136364                   1900.000000     NaN  \n",
       "39645                0.500000                      0.000000  1100.0  \n",
       "39646                     NaN                           NaN     NaN  \n",
       "39647                     NaN                           NaN     NaN  \n",
       "\n",
       "[39648 rows x 60 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "data = pd.read_csv(\"OnlineNewsPopularity.csv\", sep=r'\\s*,\\s*',header=0, encoding='ascii', engine='python')\n",
    "#url is not useful for prediction\n",
    "data = data.drop(columns= \"url\")\n",
    "data2 = data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
      "count  39647.000000    39647.000000      39647.000000     39647.000000   \n",
      "mean     354.503758       10.409085        546.462368         0.548451   \n",
      "std      214.177682        3.028510        471.121210         3.520703   \n",
      "min        0.690909        0.000000          0.000000         0.000000   \n",
      "25%      164.000000        9.000000        246.000000         0.470884   \n",
      "50%      339.000000       10.000000        409.000000         0.539249   \n",
      "75%      542.000000       12.000000        716.000000         0.608696   \n",
      "max      731.000000      442.000000       8474.000000       701.000000   \n",
      "\n",
      "       n_non_stop_words  n_non_stop_unique_tokens     num_hrefs  \\\n",
      "count      39646.000000              39644.000000  39644.000000   \n",
      "mean           0.996485                  0.808254     10.883179   \n",
      "std            5.231126                 23.801975     11.331909   \n",
      "min            0.000000                  0.000000      0.000000   \n",
      "25%            1.000000                  0.625720      4.000000   \n",
      "50%            1.000000                  0.690476      8.000000   \n",
      "75%            1.000000                  0.754650     14.000000   \n",
      "max         1042.000000               4695.000000    304.000000   \n",
      "\n",
      "       num_self_hrefs      num_imgs    num_videos  ...  min_positive_polarity  \\\n",
      "count    39644.000000  39644.000000  39644.000000  ...           39644.000000   \n",
      "mean         3.293840      4.543815      1.249977  ...               1.119175   \n",
      "std          3.855427      8.309372      4.107894  ...              19.308099   \n",
      "min          0.000000      0.000000      0.000000  ...               0.000000   \n",
      "25%          1.000000      1.000000      0.000000  ...               0.050000   \n",
      "50%          3.000000      1.000000      0.000000  ...               0.100000   \n",
      "75%          4.000000      4.000000      1.000000  ...               0.100000   \n",
      "max        116.000000    128.000000     91.000000  ...             375.000000   \n",
      "\n",
      "       max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
      "count           39644.000000           39644.000000           39643.000000   \n",
      "mean                2.278067             -11.765506             -12.061571   \n",
      "std                23.802448              60.141050              83.953733   \n",
      "min                -0.356439            -875.000000            -875.000000   \n",
      "25%                 0.600000              -0.343750              -0.800000   \n",
      "50%                 0.800000              -0.260889              -0.500000   \n",
      "75%                 1.000000              -0.192500              -0.300000   \n",
      "max               375.000000            1300.000000               0.000000   \n",
      "\n",
      "       max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
      "count           39643.000000        39641.000000              39641.000000   \n",
      "mean              -18.130971           11.464436                  2.033604   \n",
      "std                49.191453           75.742884                 51.223928   \n",
      "min              -875.000000           -0.050000               -875.000000   \n",
      "25%                -0.200000            0.000000                  0.000000   \n",
      "50%                -0.100000            0.166667                  0.000000   \n",
      "75%                -0.050000            0.500000                  0.160000   \n",
      "max              4000.000000          975.000000                725.000000   \n",
      "\n",
      "       abs_title_subjectivity  abs_title_sentiment_polarity         shares  \n",
      "count            39641.000000                  39641.000000   39640.000000  \n",
      "mean                 5.569140                      7.810715    3395.597609  \n",
      "std                 37.140687                     51.556642   11627.522322  \n",
      "min                 -0.222222                      0.000000       0.222222  \n",
      "25%                  0.166667                      0.000000     946.000000  \n",
      "50%                  0.500000                      0.000000    1400.000000  \n",
      "75%                  0.500000                      0.266667    2800.000000  \n",
      "max                475.000000                   1900.000000  843300.000000  \n",
      "\n",
      "[8 rows x 60 columns]\n",
      "timedelta                        1\n",
      "n_tokens_title                   1\n",
      "n_tokens_content                 1\n",
      "n_unique_tokens                  1\n",
      "n_non_stop_words                 2\n",
      "n_non_stop_unique_tokens         4\n",
      "num_hrefs                        4\n",
      "num_self_hrefs                   4\n",
      "num_imgs                         4\n",
      "num_videos                       4\n",
      "average_token_length             4\n",
      "num_keywords                     4\n",
      "data_channel_is_lifestyle        4\n",
      "data_channel_is_entertainment    4\n",
      "data_channel_is_bus              4\n",
      "data_channel_is_socmed           4\n",
      "data_channel_is_tech             4\n",
      "data_channel_is_world            4\n",
      "kw_min_min                       4\n",
      "kw_max_min                       4\n",
      "kw_avg_min                       4\n",
      "kw_min_max                       4\n",
      "kw_max_max                       4\n",
      "kw_avg_max                       4\n",
      "kw_min_avg                       4\n",
      "kw_max_avg                       4\n",
      "kw_avg_avg                       4\n",
      "self_reference_min_shares        4\n",
      "self_reference_max_shares        4\n",
      "self_reference_avg_sharess       4\n",
      "weekday_is_monday                4\n",
      "weekday_is_tuesday               4\n",
      "weekday_is_wednesday             4\n",
      "weekday_is_thursday              4\n",
      "weekday_is_friday                4\n",
      "weekday_is_saturday              4\n",
      "weekday_is_sunday                4\n",
      "is_weekend                       4\n",
      "LDA_00                           4\n",
      "LDA_01                           4\n",
      "LDA_02                           4\n",
      "LDA_03                           4\n",
      "LDA_04                           4\n",
      "global_subjectivity              4\n",
      "global_sentiment_polarity        4\n",
      "global_rate_positive_words       4\n",
      "global_rate_negative_words       4\n",
      "rate_positive_words              4\n",
      "rate_negative_words              4\n",
      "avg_positive_polarity            4\n",
      "min_positive_polarity            4\n",
      "max_positive_polarity            4\n",
      "avg_negative_polarity            4\n",
      "min_negative_polarity            5\n",
      "max_negative_polarity            5\n",
      "title_subjectivity               7\n",
      "title_sentiment_polarity         7\n",
      "abs_title_subjectivity           7\n",
      "abs_title_sentiment_polarity     7\n",
      "shares                           8\n",
      "dtype: int64\n",
      "timedelta                        1\n",
      "n_tokens_title                   1\n",
      "n_tokens_content                 1\n",
      "n_unique_tokens                  1\n",
      "n_non_stop_words                 2\n",
      "n_non_stop_unique_tokens         4\n",
      "num_hrefs                        4\n",
      "num_self_hrefs                   4\n",
      "num_imgs                         4\n",
      "num_videos                       4\n",
      "average_token_length             4\n",
      "num_keywords                     4\n",
      "data_channel_is_lifestyle        4\n",
      "data_channel_is_entertainment    4\n",
      "data_channel_is_bus              4\n",
      "data_channel_is_socmed           4\n",
      "data_channel_is_tech             4\n",
      "data_channel_is_world            4\n",
      "kw_min_min                       4\n",
      "kw_max_min                       4\n",
      "kw_avg_min                       4\n",
      "kw_min_max                       4\n",
      "kw_max_max                       4\n",
      "kw_avg_max                       4\n",
      "kw_min_avg                       4\n",
      "kw_max_avg                       4\n",
      "kw_avg_avg                       4\n",
      "self_reference_min_shares        4\n",
      "self_reference_max_shares        4\n",
      "self_reference_avg_sharess       4\n",
      "weekday_is_monday                4\n",
      "weekday_is_tuesday               4\n",
      "weekday_is_wednesday             4\n",
      "weekday_is_thursday              4\n",
      "weekday_is_friday                4\n",
      "weekday_is_saturday              4\n",
      "weekday_is_sunday                4\n",
      "is_weekend                       4\n",
      "LDA_00                           4\n",
      "LDA_01                           4\n",
      "LDA_02                           4\n",
      "LDA_03                           4\n",
      "LDA_04                           4\n",
      "global_subjectivity              4\n",
      "global_sentiment_polarity        4\n",
      "global_rate_positive_words       4\n",
      "global_rate_negative_words       4\n",
      "rate_positive_words              4\n",
      "rate_negative_words              4\n",
      "avg_positive_polarity            4\n",
      "min_positive_polarity            4\n",
      "max_positive_polarity            4\n",
      "avg_negative_polarity            4\n",
      "min_negative_polarity            5\n",
      "max_negative_polarity            5\n",
      "title_subjectivity               7\n",
      "title_sentiment_polarity         7\n",
      "abs_title_subjectivity           7\n",
      "abs_title_sentiment_polarity     7\n",
      "shares                           8\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.describe())\n",
    "print(data.isnull().sum())\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all null values throught simple imputer and check if is correctly applied on our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39648 entries, 0 to 39647\n",
      "Data columns (total 60 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       39648 non-null  float64\n",
      " 1   1       39648 non-null  float64\n",
      " 2   2       39648 non-null  float64\n",
      " 3   3       39648 non-null  float64\n",
      " 4   4       39648 non-null  float64\n",
      " 5   5       39648 non-null  float64\n",
      " 6   6       39648 non-null  float64\n",
      " 7   7       39648 non-null  float64\n",
      " 8   8       39648 non-null  float64\n",
      " 9   9       39648 non-null  float64\n",
      " 10  10      39648 non-null  float64\n",
      " 11  11      39648 non-null  float64\n",
      " 12  12      39648 non-null  float64\n",
      " 13  13      39648 non-null  float64\n",
      " 14  14      39648 non-null  float64\n",
      " 15  15      39648 non-null  float64\n",
      " 16  16      39648 non-null  float64\n",
      " 17  17      39648 non-null  float64\n",
      " 18  18      39648 non-null  float64\n",
      " 19  19      39648 non-null  float64\n",
      " 20  20      39648 non-null  float64\n",
      " 21  21      39648 non-null  float64\n",
      " 22  22      39648 non-null  float64\n",
      " 23  23      39648 non-null  float64\n",
      " 24  24      39648 non-null  float64\n",
      " 25  25      39648 non-null  float64\n",
      " 26  26      39648 non-null  float64\n",
      " 27  27      39648 non-null  float64\n",
      " 28  28      39648 non-null  float64\n",
      " 29  29      39648 non-null  float64\n",
      " 30  30      39648 non-null  float64\n",
      " 31  31      39648 non-null  float64\n",
      " 32  32      39648 non-null  float64\n",
      " 33  33      39648 non-null  float64\n",
      " 34  34      39648 non-null  float64\n",
      " 35  35      39648 non-null  float64\n",
      " 36  36      39648 non-null  float64\n",
      " 37  37      39648 non-null  float64\n",
      " 38  38      39648 non-null  float64\n",
      " 39  39      39648 non-null  float64\n",
      " 40  40      39648 non-null  float64\n",
      " 41  41      39648 non-null  float64\n",
      " 42  42      39648 non-null  float64\n",
      " 43  43      39648 non-null  float64\n",
      " 44  44      39648 non-null  float64\n",
      " 45  45      39648 non-null  float64\n",
      " 46  46      39648 non-null  float64\n",
      " 47  47      39648 non-null  float64\n",
      " 48  48      39648 non-null  float64\n",
      " 49  49      39648 non-null  float64\n",
      " 50  50      39648 non-null  float64\n",
      " 51  51      39648 non-null  float64\n",
      " 52  52      39648 non-null  float64\n",
      " 53  53      39648 non-null  float64\n",
      " 54  54      39648 non-null  float64\n",
      " 55  55      39648 non-null  float64\n",
      " 56  56      39648 non-null  float64\n",
      " 57  57      39648 non-null  float64\n",
      " 58  58      39648 non-null  float64\n",
      " 59  59      39648 non-null  float64\n",
      "dtypes: float64(60)\n",
      "memory usage: 18.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPUTER AND REMOVE NONE WITH STRATEGY Mean\n",
    "data = pd.DataFrame(data)\n",
    "s_imputer = SimpleImputer(missing_values=np.nan,strategy=\"mean\")\n",
    "data = s_imputer.fit_transform(data)\n",
    "data = pd.DataFrame(data)\n",
    "data.info()\n",
    "#Verify that the length og the dataset is equal to the non null values \n",
    "index = data.index\n",
    "number_of_rows = len(index)\n",
    "number_of_rows == 39648"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on column: 0\n",
      "Working on column: 1\n",
      "Working on column: 2\n",
      "Working on column: 3\n",
      "Working on column: 4\n",
      "Working on column: 5\n",
      "Working on column: 6\n",
      "Working on column: 7\n",
      "Working on column: 8\n",
      "Working on column: 9\n",
      "Working on column: 10\n",
      "Working on column: 11\n",
      "Working on column: 12\n",
      "Working on column: 13\n",
      "Working on column: 14\n",
      "Working on column: 15\n",
      "Working on column: 16\n",
      "Working on column: 17\n",
      "Working on column: 18\n",
      "Working on column: 19\n",
      "Working on column: 20\n",
      "Working on column: 21\n",
      "Working on column: 22\n",
      "Working on column: 23\n",
      "Working on column: 24\n",
      "Working on column: 25\n",
      "Working on column: 26\n",
      "Working on column: 27\n",
      "Working on column: 28\n",
      "Working on column: 29\n",
      "Working on column: 30\n",
      "Working on column: 31\n",
      "Working on column: 32\n",
      "Working on column: 33\n",
      "Working on column: 34\n",
      "Working on column: 35\n",
      "Working on column: 36\n",
      "Working on column: 37\n",
      "Working on column: 38\n",
      "Working on column: 39\n",
      "Working on column: 40\n",
      "Working on column: 41\n",
      "Working on column: 42\n",
      "Working on column: 43\n",
      "Working on column: 44\n",
      "Working on column: 45\n",
      "Working on column: 46\n",
      "Working on column: 47\n",
      "Working on column: 48\n",
      "Working on column: 49\n",
      "Working on column: 50\n",
      "Working on column: 51\n",
      "Working on column: 52\n",
      "Working on column: 53\n",
      "Working on column: 54\n",
      "Working on column: 55\n",
      "Working on column: 56\n",
      "Working on column: 57\n",
      "Working on column: 58\n",
      "Working on column: 59\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_outliers(df,columns,n_std):\n",
    "    for col in columns:\n",
    "        print('Working on column: {}'.format(col))\n",
    "        \n",
    "        mean = df[col].mean()\n",
    "        sd = df[col].std()\n",
    "        \n",
    "        df = df[(df[col] <= mean+(n_std*sd))]\n",
    "        \n",
    "    return df\n",
    "\n",
    "data = remove_outliers(data,data.columns,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the Dataset with MinMax Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import KBinsDiscretizer \n",
    "#data3 = data\n",
    "scaler = MinMaxScaler()\n",
    "#print(X.head())\n",
    "x_scal = scaler.fit_transform(data)\n",
    "data = DataFrame(x_scal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.111963</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>0.999467</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.498750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.015835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.130368</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.999864</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.107873</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.999467</td>\n",
       "      <td>0.999086</td>\n",
       "      <td>0.999644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.271472</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.999577</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>0.999556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.189162</td>\n",
       "      <td>0.559889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.698198</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.777143</td>\n",
       "      <td>0.999543</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.501429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.022879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.313906</td>\n",
       "      <td>0.476033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.614987</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.999793</td>\n",
       "      <td>0.999657</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.244376</td>\n",
       "      <td>0.514039</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.641844</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.999744</td>\n",
       "      <td>0.999543</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.832822</td>\n",
       "      <td>0.425711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.606092</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.999509</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.496000</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.037531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.176892</td>\n",
       "      <td>0.529052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21613</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.348671</td>\n",
       "      <td>0.539493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692661</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.999765</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21614 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0       1         2         3    4         5         6   \\\n",
       "0      1.000000  0.5625  0.111963  0.663594  1.0  0.815385  0.095238   \n",
       "1      1.000000  0.3750  0.130368  0.604743  1.0  0.791946  0.071429   \n",
       "2      1.000000  0.3750  0.107873  0.575130  1.0  0.663866  0.071429   \n",
       "3      1.000000  0.3750  0.271472  0.503788  1.0  0.665635  0.214286   \n",
       "4      1.000000  0.4375  0.189162  0.559889  1.0  0.698198  0.047619   \n",
       "...         ...     ...       ...       ...  ...       ...       ...   \n",
       "21609  0.001383  0.3750  0.313906  0.476033  1.0  0.614987  0.238095   \n",
       "21610  0.001383  0.6250  0.244376  0.514039  1.0  0.641844  0.428571   \n",
       "21611  0.000000  0.6250  0.832822  0.425711  1.0  0.606092  0.357143   \n",
       "21612  0.000000  0.5000  0.176892  0.529052  1.0  0.684783  0.214286   \n",
       "21613  0.000000  0.1875  0.348671  0.539493  1.0  0.692661  0.238095   \n",
       "\n",
       "             7     8         9   ...        50    51        52        53  \\\n",
       "0      0.166667  0.04  0.000000  ...  0.100000  0.70  0.999600  0.999314   \n",
       "1      0.083333  0.04  0.000000  ...  0.033333  0.70  0.999864  0.857143   \n",
       "2      0.083333  0.04  0.000000  ...  0.100000  1.00  0.999467  0.999086   \n",
       "3      0.000000  0.04  0.000000  ...  0.136364  0.80  0.999577  0.999314   \n",
       "4      0.166667  0.00  0.000000  ...  0.136364  0.60  0.777143  0.999543   \n",
       "...         ...   ...       ...  ...       ...   ...       ...       ...   \n",
       "21609  0.000000  0.04  0.000000  ...  0.050000  0.80  0.999793  0.999657   \n",
       "21610  0.166667  0.08  0.000000  ...  0.033333  1.00  0.999744  0.999543   \n",
       "21611  1.000000  0.24  0.000000  ...  0.033333  1.00  0.999509  0.998857   \n",
       "21612  0.583333  0.04  0.090909  ...  0.100000  0.75  0.999703  0.999429   \n",
       "21613  0.083333  0.04  0.000000  ...  0.062500  0.50  0.999765  0.999429   \n",
       "\n",
       "             54        55        56        57        58        59  \n",
       "0      0.999467  0.500000  0.498750  0.000000  0.002500  0.015835  \n",
       "1      0.999733  0.000000  0.500000  1.000000  0.000000  0.019007  \n",
       "2      0.999644  0.000000  0.500000  1.000000  0.000000  0.040219  \n",
       "3      0.999556  0.000000  0.500000  1.000000  0.000000  0.032154  \n",
       "4      0.999733  0.642857  0.501429  0.285714  0.002857  0.022879  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "21609  0.999867  0.000000  0.500000  1.000000  0.000000  0.040219  \n",
       "21610  0.999867  0.100000  0.500000  0.800000  0.000000  0.034842  \n",
       "21611  0.999867  0.783333  0.496000  0.566667  0.008000  0.037531  \n",
       "21612  0.666667  0.100000  0.500000  0.800000  0.000000  0.048285  \n",
       "21613  0.999967  0.000000  0.500000  1.000000  0.000000  0.029466  \n",
       "\n",
       "[21614 rows x 60 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your comments for step 2.1 Clean and Load the Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Dataset Analysis (up to 1 of 11.2 points)\n",
    "In the following code cell (feel free to create new cells), remember to comment your code snippets:\n",
    "\n",
    "1) Print the total number of samples;\n",
    "\n",
    "2) Print a table with the first 15 samples;\n",
    "\n",
    "3) Plot the histogram distribution of \"shares\";\n",
    "\n",
    "4) A bar chart counting the attributes:  data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Total Number of Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39648\n"
     ]
    }
   ],
   "source": [
    "print(number_of_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Table with 15 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.111963</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>0.999467</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.498750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.015835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.130368</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.999864</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.107873</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999467</td>\n",
       "      <td>0.999086</td>\n",
       "      <td>0.999644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.271472</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.999577</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>0.999556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.189162</td>\n",
       "      <td>0.559889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.698198</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.777143</td>\n",
       "      <td>0.999543</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.501429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.022879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.118098</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.999728</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.638037</td>\n",
       "      <td>0.490050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.731638</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999526</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.140082</td>\n",
       "      <td>0.609195</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.707602</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.999645</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.022018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.132413</td>\n",
       "      <td>0.562753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.999841</td>\n",
       "      <td>0.999786</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.503667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.020352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.348671</td>\n",
       "      <td>0.459542</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.634961</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.999748</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.498333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.042908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.202965</td>\n",
       "      <td>0.624679</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.805668</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999835</td>\n",
       "      <td>0.999771</td>\n",
       "      <td>0.999733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.199898</td>\n",
       "      <td>0.510256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.999876</td>\n",
       "      <td>0.999810</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.124744</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.680272</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.999478</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.497333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.022798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.135992</td>\n",
       "      <td>0.572581</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.721088</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.999771</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.502667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.020943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.169223</td>\n",
       "      <td>0.562691</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999698</td>\n",
       "      <td>0.999429</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0       1         2         3    4         5         6         7     8   \\\n",
       "0   1.0  0.5625  0.111963  0.663594  1.0  0.815385  0.095238  0.166667  0.04   \n",
       "1   1.0  0.3750  0.130368  0.604743  1.0  0.791946  0.071429  0.083333  0.04   \n",
       "2   1.0  0.3750  0.107873  0.575130  1.0  0.663866  0.071429  0.083333  0.04   \n",
       "3   1.0  0.3750  0.271472  0.503788  1.0  0.665635  0.214286  0.000000  0.04   \n",
       "4   1.0  0.4375  0.189162  0.559889  1.0  0.698198  0.047619  0.166667  0.00   \n",
       "5   1.0  0.4375  0.118098  0.636364  1.0  0.797101  0.095238  0.083333  0.04   \n",
       "6   1.0  0.3750  0.638037  0.490050  1.0  0.731638  0.261905  0.000000  0.04   \n",
       "7   1.0  0.3750  0.140082  0.609195  1.0  0.707602  0.428571  0.166667  0.44   \n",
       "8   1.0  0.3125  0.132413  0.562753  1.0  0.644444  0.452381  0.250000  0.36   \n",
       "9   1.0  0.5625  0.348671  0.459542  1.0  0.634961  0.238095  0.000000  0.04   \n",
       "10  1.0  0.3125  0.202965  0.624679  1.0  0.805668  0.261905  0.000000  0.04   \n",
       "11  1.0  0.3750  0.199898  0.510256  1.0  0.650000  0.214286  0.166667  0.04   \n",
       "12  1.0  0.6250  0.124744  0.560000  1.0  0.680272  0.071429  0.166667  0.04   \n",
       "13  1.0  0.3125  0.135992  0.572581  1.0  0.721088  0.119048  0.166667  0.04   \n",
       "14  1.0  0.3125  0.169223  0.562691  1.0  0.724490  0.119048  0.250000  0.04   \n",
       "\n",
       "          9   ...        50        51        52        53        54        55  \\\n",
       "0   0.000000  ...  0.100000  0.700000  0.999600  0.999314  0.999467  0.500000   \n",
       "1   0.000000  ...  0.033333  0.700000  0.999864  0.857143  0.999733  0.000000   \n",
       "2   0.000000  ...  0.100000  1.000000  0.999467  0.999086  0.999644  0.000000   \n",
       "3   0.000000  ...  0.136364  0.800000  0.999577  0.999314  0.999556  0.000000   \n",
       "4   0.000000  ...  0.136364  0.600000  0.777143  0.999543  0.999733  0.642857   \n",
       "5   0.090909  ...  0.100000  0.500000  0.999728  0.999429  0.999733  0.000000   \n",
       "6   0.000000  ...  0.100000  1.000000  0.999526  0.998857  0.999733  0.000000   \n",
       "7   0.000000  ...  0.200000  0.700000  0.999645  0.999314  0.999867  1.000000   \n",
       "8   0.000000  ...  0.136364  0.500000  0.999841  0.999786  0.999867  0.750000   \n",
       "9   0.000000  ...  0.050000  0.600000  0.999748  0.999314  0.999867  0.750000   \n",
       "10  0.000000  ...  0.033333  1.000000  0.999835  0.999771  0.999733  0.000000   \n",
       "11  0.090909  ...  0.050000  0.350000  0.999876  0.999810  0.999867  0.000000   \n",
       "12  0.000000  ...  0.136364  0.433333  0.999478  0.998857  0.666667  0.700000   \n",
       "13  0.000000  ...  0.250000  0.350000  0.999867  0.999771  0.999867  0.900000   \n",
       "14  0.000000  ...  0.100000  1.000000  0.999698  0.999429  0.666667  0.000000   \n",
       "\n",
       "          56        57        58        59  \n",
       "0   0.498750  0.000000  0.002500  0.015835  \n",
       "1   0.500000  1.000000  0.000000  0.019007  \n",
       "2   0.500000  1.000000  0.000000  0.040219  \n",
       "3   0.500000  1.000000  0.000000  0.032154  \n",
       "4   0.501429  0.285714  0.002857  0.022879  \n",
       "5   0.500000  1.000000  0.000000  0.018981  \n",
       "6   0.500000  1.000000  0.000000  0.059039  \n",
       "7   0.493333  1.000000  0.013333  0.022018  \n",
       "8   0.503667  0.500000  0.007333  0.020352  \n",
       "9   0.498333  0.500000  0.003333  0.042908  \n",
       "10  0.500000  1.000000  0.000000  0.083235  \n",
       "11  0.500000  1.000000  0.000000  0.015969  \n",
       "12  0.497333  0.400000  0.005333  0.022798  \n",
       "13  0.502667  0.800000  0.005333  0.020943  \n",
       "14  0.500000  1.000000  0.000000  0.040219  \n",
       "\n",
       "[15 rows x 60 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot no-discretize share columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#data['shares']\n",
    "data[59]\n",
    "plt.hist(data[59])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bar chart counting the attributes: data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alessandro\\AppData\\Local\\Temp\\ipykernel_21452\\1666948250.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new = new.append({'features': f, 'samples': data2[data2[f]==1].shape[0]}, ignore_index=True)\n",
      "C:\\Users\\Alessandro\\AppData\\Local\\Temp\\ipykernel_21452\\1666948250.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new = new.append({'features': f, 'samples': data2[data2[f]==1].shape[0]}, ignore_index=True)\n",
      "C:\\Users\\Alessandro\\AppData\\Local\\Temp\\ipykernel_21452\\1666948250.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new = new.append({'features': f, 'samples': data2[data2[f]==1].shape[0]}, ignore_index=True)\n",
      "C:\\Users\\Alessandro\\AppData\\Local\\Temp\\ipykernel_21452\\1666948250.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new = new.append({'features': f, 'samples': data2[data2[f]==1].shape[0]}, ignore_index=True)\n",
      "C:\\Users\\Alessandro\\AppData\\Local\\Temp\\ipykernel_21452\\1666948250.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new = new.append({'features': f, 'samples': data2[data2[f]==1].shape[0]}, ignore_index=True)\n",
      "C:\\Users\\Alessandro\\AppData\\Local\\Temp\\ipykernel_21452\\1666948250.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new = new.append({'features': f, 'samples': data2[data2[f]==1].shape[0]}, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAGaCAYAAADjOJNrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAzbElEQVR4nO3dfbyUdZ3/8dcbJFADU0FToMCWVARFREKt1jKFysS12MUy2bJI19Jqa4PKci12bW/cspTNzRV0NxU103Uzb7DUzBUPeMONkayiEq6Q/VTURMHP74/rOzAcDufMgTlzzVzX+/l4nMfMfOe6znyGw/WZ73xvFRGYmVk59Mo7ADMzaxwnfTOzEnHSNzMrESd9M7MScdI3MysRJ30zsxLZKe8AujJw4MAYNmxY3mGYmbWUhQsX/j4iBrUvb/qkP2zYMNra2vIOw8yspUh6oqNyN++YmZWIk76ZWYk46ZuZlUjTt+l35LXXXmPVqlW88soreYfSEvr168eQIUPo06dP3qGYWc5aMumvWrWK/v37M2zYMCTlHU5TiwieffZZVq1axfDhw/MOx8xy1pLNO6+88gp77rmnE34NJLHnnnv6W5GZAS2a9AEn/G7wv5WZVbRs0i+Lo48+2vMUzKxuWrJNv71hM/67rr9v5fkfrOvvM7PiqHe+6Uq985Fr+tvhpZde4oMf/CCHHHIIo0aN4uqrr+a8887j8MMPZ9SoUUyfPp3KjmRHH300X/jCF3j3u9/NgQceyP33389JJ53EiBEj+PrXvw7AypUrOeCAA5g2bRoHH3wwH/nIR3j55Ze3et1bb72VI444grFjxzJlyhRefPFFAGbMmMHIkSM5+OCD+dKXvtS4fwgzazlO+tvh5z//Ofvuuy8PPfQQS5YsYdKkSXz2s5/l/vvvZ8mSJfzxj3/kpptu2nT8G97wBu666y5OP/10Jk+ezEUXXcSSJUuYM2cOzz77LADLly9n+vTpPPzwwwwYMICLL754i9f8/e9/z7e//W1uv/12Fi1axLhx47jgggv4wx/+wPXXX8/SpUt5+OGHN32QmJl1xEl/O4wePZrbb7+dr3zlK9x9993stttu/OIXv+Ad73gHo0eP5o477mDp0qWbjj/hhBM2nXfQQQexzz770LdvX/bbbz+eeuopAIYOHcpRRx0FwCmnnMKvfvWrLV7zf/7nf1i2bBlHHXUUY8aMYe7cuTzxxBMMGDCAfv368alPfYqf/OQn7LLLLg36VzCzVlSINv1Ge/vb387ChQv52c9+xsyZMznuuOO46KKLaGtrY+jQoZx77rlbDJHs27cvAL169dp0v/J4w4YNwNYjbNo/jgiOPfZYrrzyyq3iWbBgAfPnz+eqq67iBz/4AXfccUfd3quZFYtr+tth9erV7LLLLpxyyil86UtfYtGiRQAMHDiQF198kWuvvbbbv/PJJ5/k3nvvBeDKK6/kne985xbPT5gwgXvuuYcVK1YA8PLLL/Pb3/6WF198keeff54PfOADfPe73+XBBx/csTdnZoXmmv52WLx4MV/+8pfp1asXffr0Yfbs2fz0pz9l9OjRDBs2jMMPP7zbv/PAAw9k7ty5fOYzn2HEiBGcccYZWzw/aNAg5syZw8knn8z69esB+Pa3v03//v2ZPHkyr7zyChHBv/zLv9TlPZpZMakyyqTTg6QvAJ8CAlgMfALYBbgaGAasBP48Iv5fOn4mcBqwETgrIm5J5YcBc4CdgZ8BZ0cXAYwbNy7aj1N/5JFHOPDAA2t8i81v5cqVHH/88SxZsqTHXqNo/2ZmeWmVIZuSFkbEuPblXTbvSBoMnAWMi4hRQG9gKjADmB8RI4D56TGSRqbnDwImARdL6p1+3WxgOjAi/UzarndjZmbbpdY2/Z2AnSXtRFbDXw1MBuam5+cCJ6b7k4GrImJ9RDwOrADGS9oHGBAR96ba/eVV55TasGHDerSWb2ZW0WXSj4jfAf8EPAk8DTwfEbcCe0fE0+mYp4G90imDgaeqfsWqVDY43W9fbmZmDVJL887uZLX34cC+wK6STunslA7KopPyjl5zuqQ2SW1r167t8EVq6YuwjP+tzKyiluad9wGPR8TaiHgN+AlwJPBMarIh3a5Jx68ChladP4SsOWhVut++fCsRcUlEjIuIcYMGbbWZO/369ePZZ591MqtBZT39fv365R2KmTWBWoZsPglMkLQL8EfgGKANeAmYBpyfbm9Ix98I/FjSBWTfDEYACyJio6R1kiYA9wGnAt/fnqCHDBnCqlWr2Na3ANtSZecsM7Muk35E3CfpWmARsAF4ALgEeCMwT9JpZB8MU9LxSyXNA5al48+MiI3p153B5iGbN6efbuvTp493gTIz2w41Tc6KiG8C32xXvJ6s1t/R8bOAWR2UtwGjuhmjmZnViZdhMDMrESd9M7MScdI3MysRJ30zsxJx0jczKxEnfTOzEnHSNzMrESd9M7MScdI3MysRJ30zsxJx0jczKxFvjG5mddUqe8iWlWv6ZmYl4qRvZlYiTvpmZiXipG9mViK1bIy+v6QHq35ekPR5SXtIuk3So+l296pzZkpaIWm5pIlV5YdJWpyeu1BSR5ulm5lZD+ky6UfE8ogYExFjgMOAl4HrgRnA/IgYAcxPj5E0EpgKHARMAi6W1Dv9utnAdLJ9c0ek583MrEG6O2TzGOB/I+IJSZOBo1P5XOCXwFeAycBVEbEeeFzSCmC8pJXAgIi4F0DS5cCJbOc+uWXmIXFmtr2626Y/Fbgy3d87Ip4GSLd7pfLBwFNV56xKZYPT/fblZmbWIDUnfUlvAE4Arunq0A7KopPyjl5ruqQ2SW1r166tNUQzM+tCd2r67wcWRcQz6fEzkvYBSLdrUvkqYGjVeUOA1al8SAflW4mISyJiXESMGzRoUDdCNDOzznQn6Z/M5qYdgBuBaen+NOCGqvKpkvpKGk7WYbsgNQGtkzQhjdo5teocMzNrgJo6ciXtAhwLfKaq+HxgnqTTgCeBKQARsVTSPGAZsAE4MyI2pnPOAOYAO5N14LoT18ysgWpK+hHxMrBnu7JnyUbzdHT8LGBWB+VtwKjuh2lmZvXgGblmZiXipG9mViJO+mZmJeKkb2ZWIk76ZmYl4qRvZlYiTvpmZiXipG9mViJO+mZmJeKkb2ZWIk76ZmYl4qRvZlYiTvpmZiXS3T1yzXqc9wA26zmu6ZuZlUhNSV/SmyRdK+k3kh6RdISkPSTdJunRdLt71fEzJa2QtFzSxKrywyQtTs9dmHbQMjOzBqm1pv894OcRcQBwCPAIMAOYHxEjgPnpMZJGAlOBg4BJwMWSeqffMxuYTraF4oj0vJmZNUiXSV/SAODdwKUAEfFqRDwHTAbmpsPmAiem+5OBqyJifUQ8DqwAxqfN0wdExL0REcDlVeeYmVkD1FLT3w9YC1wm6QFJP5K0K7B32uycdLtXOn4w8FTV+atS2eB0v325mZk1SC1JfydgLDA7Ig4FXiI15WxDR+300Un51r9Ami6pTVLb2rVrawjRzMxqUUvSXwWsioj70uNryT4EnklNNqTbNVXHD606fwiwOpUP6aB8KxFxSUSMi4hxgwYNqvW9mJlZF7pM+hHxf8BTkvZPRccAy4AbgWmpbBpwQ7p/IzBVUl9Jw8k6bBekJqB1kiakUTunVp1jZmYNUOvkrM8B/ynpDcBjwCfIPjDmSToNeBKYAhARSyXNI/tg2ACcGREb0+85A5gD7AzcnH7MzKxBakr6EfEgMK6Dp47ZxvGzgFkdlLcBo7oRn5mZ1ZFn5JqZlYiTvplZiTjpm5mViJO+mVmJOOmbmZWIk76ZWYk46ZuZlYiTvplZiTjpm5mViJO+mVmJOOmbmZWIk76ZWYk46ZuZlYiTvplZiTjpm5mViJO+mVmJ1JT0Ja2UtFjSg5LaUtkekm6T9Gi63b3q+JmSVkhaLmliVflh6feskHRh2jbRzMwapDs1/fdExJiIqOygNQOYHxEjgPnpMZJGAlOBg4BJwMWSeqdzZgPTyfbNHZGeNzOzBtmR5p3JwNx0fy5wYlX5VRGxPiIeB1YA4yXtAwyIiHsjIoDLq84xM7MGqDXpB3CrpIWSpqeyvSPiaYB0u1cqHww8VXXuqlQ2ON1vX25mZg1S08bowFERsVrSXsBtkn7TybEdtdNHJ+Vb/4Lsg2U6wFve8pYaQzQzs67UVNOPiNXpdg1wPTAeeCY12ZBu16TDVwFDq04fAqxO5UM6KO/o9S6JiHERMW7QoEG1vxszM+tUl0lf0q6S+lfuA8cBS4AbgWnpsGnADen+jcBUSX0lDSfrsF2QmoDWSZqQRu2cWnWOmZk1QC3NO3sD16fRlTsBP46In0u6H5gn6TTgSWAKQEQslTQPWAZsAM6MiI3pd50BzAF2Bm5OP2Zm1iBdJv2IeAw4pIPyZ4FjtnHOLGBWB+VtwKjuh2lmZvXgGblmZiXipG9mViJO+mZmJeKkb2ZWIk76ZmYl4qRvZlYiTvpmZiXipG9mViJO+mZmJeKkb2ZWIk76ZmYl4qRvZlYiTvpmZiXipG9mViJO+mZmJVJz0pfUW9IDkm5Kj/eQdJukR9Pt7lXHzpS0QtJySROryg+TtDg9d2HaQcvMzBqkOzX9s4FHqh7PAOZHxAhgfnqMpJHAVOAgYBJwsaTe6ZzZZBuej0g/k3YoejMz65aakr6kIcAHgR9VFU8G5qb7c4ETq8qvioj1EfE4sAIYnzZPHxAR90ZEAJdXnWNmZg1Qa03/u8DfAK9Xle2dNjsn3e6VygcDT1UdtyqVDU7325ebmVmDdJn0JR0PrImIhTX+zo7a6aOT8o5ec7qkNklta9eurfFlzcysK7XU9I8CTpC0ErgKeK+k/wCeSU02pNs16fhVwNCq84cAq1P5kA7KtxIRl0TEuIgYN2jQoG68HTMz60yXST8iZkbEkIgYRtZBe0dEnALcCExLh00Dbkj3bwSmSuoraThZh+2C1AS0TtKENGrn1KpzzMysAXbagXPPB+ZJOg14EpgCEBFLJc0DlgEbgDMjYmM65wxgDrAzcHP6MTOzBulW0o+IXwK/TPefBY7ZxnGzgFkdlLcBo7obpJmZ1Ydn5JqZlYiTvplZiTjpm5mViJO+mVmJOOmbmZWIk76ZWYk46ZuZlYiTvplZiTjpm5mViJO+mVmJOOmbmZWIk76ZWYk46ZuZlYiTvplZiTjpm5mViJO+mVmJ1LIxej9JCyQ9JGmppL9N5XtIuk3So+l296pzZkpaIWm5pIlV5YdJWpyeuzBtm2hmZg1SS01/PfDeiDgEGANMkjQBmAHMj4gRwPz0GEkjyfbSPQiYBFwsqXf6XbOB6WT75o5Iz5uZWYPUsjF6RMSL6WGf9BPAZGBuKp8LnJjuTwauioj1EfE4sAIYL2kfYEBE3BsRAVxedY6ZmTVATW36knpLehBYA9wWEfcBe0fE0wDpdq90+GDgqarTV6Wywel++3IzM2uQmpJ+RGyMiDHAELJae2ebm3fUTh+dlG/9C6Tpktokta1du7aWEM3MrAbdGr0TEc8BvyRri38mNdmQbtekw1YBQ6tOGwKsTuVDOijv6HUuiYhxETFu0KBB3QnRzMw6UcvonUGS3pTu7wy8D/gNcCMwLR02Dbgh3b8RmCqpr6ThZB22C1IT0DpJE9KonVOrzjEzswbYqYZj9gHmphE4vYB5EXGTpHuBeZJOA54EpgBExFJJ84BlwAbgzIjYmH7XGcAcYGfg5vRjZmYN0mXSj4iHgUM7KH8WOGYb58wCZnVQ3gZ01h9gZmY9yDNyzcxKxEnfzKxEnPTNzErESd/MrESc9M3MSsRJ38ysRJz0zcxKxEnfzKxEnPTNzErESd/MrESc9M3MSqSWBdfMrI6Gzfjvhr7eyvM/2NDXs+bmmr6ZWYk46ZuZlYiTvplZidSyc9ZQSb+Q9IikpZLOTuV7SLpN0qPpdveqc2ZKWiFpuaSJVeWHSVqcnrsw7aBlZmYNUktNfwPw1xFxIDABOFPSSGAGMD8iRgDz02PSc1OBg8j20r047boFMBuYTraF4oj0vJmZNUgtO2c9DTyd7q+T9AgwGJgMHJ0Om0u2YfpXUvlVEbEeeFzSCmC8pJXAgIi4F0DS5cCJ9NCWiR4hYWa2tW616UsaRrZ14n3A3ukDofLBsFc6bDDwVNVpq1LZ4HS/fbmZmTVIzUlf0huB64DPR8QLnR3aQVl0Ut7Ra02X1Capbe3atbWGaGZmXagp6UvqQ5bw/zMifpKKn5G0T3p+H2BNKl8FDK06fQiwOpUP6aB8KxFxSUSMi4hxgwYNqvW9mJlZF2oZvSPgUuCRiLig6qkbgWnp/jTghqryqZL6ShpO1mG7IDUBrZM0If3OU6vOMTOzBqhlGYajgI8DiyU9mMq+CpwPzJN0GvAkMAUgIpZKmgcsIxv5c2ZEbEznnQHMAXYm68DtkU5cMzPrWC2jd35Fx+3xAMds45xZwKwOytuAUd0J0MzM6sczcs3MSsRJ38ysRJz0zcxKxEnfzKxEnPTNzErESd/MrESc9M3MSsRJ38ysRJz0zcxKxEnfzKxEnPTNzErESd/MrESc9M3MSsRJ38ysRJz0zcxKxEnfzKxEatku8d8lrZG0pKpsD0m3SXo03e5e9dxMSSskLZc0sar8MEmL03MXpi0TzcysgWqp6c8BJrUrmwHMj4gRwPz0GEkjganAQemciyX1TufMBqaT7Zk7ooPfaWZmPazLpB8RdwF/aFc8GZib7s8FTqwqvyoi1kfE48AKYLykfYABEXFvRARwedU5ZmbWINvbpr93RDwNkG73SuWDgaeqjluVygan++3LzcysgerdkdtRO310Ut7xL5GmS2qT1LZ27dq6BWdmVnbbm/SfSU02pNs1qXwVMLTquCHA6lQ+pIPyDkXEJRExLiLGDRo0aDtDNDOz9rY36d8ITEv3pwE3VJVPldRX0nCyDtsFqQlonaQJadTOqVXnmJlZg+zU1QGSrgSOBgZKWgV8EzgfmCfpNOBJYApARCyVNA9YBmwAzoyIjelXnUE2Emhn4Ob0Y2ZmDdRl0o+Ik7fx1DHbOH4WMKuD8jZgVLeiMzOzuvKMXDOzEnHSNzMrESd9M7MScdI3MysRJ30zsxJx0jczKxEnfTOzEnHSNzMrESd9M7MScdI3MysRJ30zsxJx0jczKxEnfTOzEnHSNzMrESd9M7MScdI3MyuRhid9SZMkLZe0QtKMRr++mVmZNTTpS+oNXAS8HxgJnCxpZCNjMDMrs0bX9McDKyLisYh4FbgKmNzgGMzMSqvRSX8w8FTV41WpzMzMGkAR0bgXk6YAEyPiU+nxx4HxEfG5dsdNB6anh/sDyxsWJAwEft/A12ukIr838PtrdX5/9fXWiBjUvnCnBgYAWc1+aNXjIcDq9gdFxCXAJY0KqpqktogYl8dr97Qivzfw+2t1fn+N0ejmnfuBEZKGS3oDMBW4scExmJmVVkNr+hGxQdJngVuA3sC/R8TSRsZgZlZmjW7eISJ+Bvys0a/bDbk0KzVIkd8b+P21Or+/BmhoR66ZmeXLyzCYmZWIk76ZWYk46RecpL61lJlZOTS8I7dZSXorMCIibpe0M7BTRKzLO646uBcYW0NZS0oT/n4eEeskfZ3sfX07IhblHNoOkXRSZ89HxE8aFUtPk3QkMIyqfBQRl+cWUME56QOSPk02A3gP4G1kk8b+FTgmz7h2hKQ3ky1xsbOkQwGlpwYAu+QWWP2dExHXSHonMBH4J2A28I58w9phH0q3ewFHAnekx+8BfgkUIulLuoLsmnsQ2JiKA2jppC9pHdn76FBEDGhgOFtw0s+cSbYY3H0AEfGopL3yDWmHTQT+kuwD7IKq8nXAV/MIqIdUEsUHgdkRcYOkc3OMpy4i4hMAkm4CRkbE0+nxPmQr1RbFOLL3V6hhhBHRH0DSecD/AVeQVbw+BvTPMTQn/WR9RLwqZZVhSTvRyad0K4iIucBcSR+OiOvyjqcH/U7SD4H3Ad9J/RVF6qsaVkn4yTPA2/MKpgcsAd4MPN3VgS1qYkRUf+ucLek+4B/yCshJP3OnpK+SNYUcC/wV8F85x1QvN0n6KFu3mZ6XW0T19efAJOCfIuK5VBP+cs4x1dMvJd0CXElWEZkK/CLfkHacpP8iez/9gWWSFgDrK89HxAl5xVZnGyV9jGwZ+QBOZvO301x4chYgqRdwGnAc2VewW4AfFeErp6SfA88DC6n6zxYR/5xbUHUk6S0dlUfEk42OpadI+jPg3enhXRFxfZ7x1IOkP+3s+Yi4s1Gx9CRJw4DvAUeRJf17gM9HxMrcYipAXrNOSFoSEaPyjqOnSFpMdjEJ6AcMB5ZHxEG5BlZH7UaW7QL0LsjIMiQNB56OiFfS452BvfNMikVX6uadqoTRoYg4uIHh9JRfSxodEYvzDqQnRMTo6seSxgKfySmcuutgZNlgWnxkWTvXkI1OqtiYyg7PJ5z6kPR9Os8tZzUwnC2UOukDx+cdQAO8E/hLSY+TtZkKiIJ8oG0lIhZJaumE0U4RR5ZV2yltnQpAGlDxhjwDqpO2vAPYllIn/Yh4AiAt9/wfEfFcvhH1iPfnHUBPkvTFqoe9yCZnrc0pnJ5QuJFl7ayVdEJE3AggaTIF2D0rIuZK6g2cHxFNNbCgSEPbdsSbgTZJ8yRNUuUKK4D0wTYUeG+6/zLF+rv3r/rpC/w3MDnXiOqr/ciyayjOyDKA04GvSnpK0pPAVyhI81xEbAQOyzuO9tyRm6REfxzwCbIJI/OASyPif3MNbAdJ+ibZ+9k/It4uaV/gmog4KufQ6krSALJmq0J0cFYUeWRZNUlvJMtHRfv7/TMwguzD+qVKeZ7LaJS6eadaRISk/yObPbcB2B24VtJtEfE3+Ua3Q/4MOBRYBBARqyXlOiOwniSNAy4jzXKU9DzwyYhYmGtgdRIRrwP/ln4KR9LewN8B+0bE+yWNBI6IiEtzDq1e9gCeBd5bVRbkuIyGa/qApLOAaWRtiT8CfhoRr6Va1qMR8bZcA9wBkhZExHhJiyJirKRdgXuL0pEr6WHgzIi4Oz1+J3Bxgd7f8cC3gLeSVdIqHfG5rd1ST5JuJvvQ/lpEHJL6LB5oPyrL6qdIbbs7YiBwUkRMjIhrIuI12FTLavURPvPSMgVvSsP/bqdYtcZ1lYQPEBG/IltfqCi+S1Yh2TMiBkRE/6Ik/GRgRMwDXodsH21ynrFaT5KGSLpe0hpJz0i6TtKQPGNy805meGUkT4WkKyLi4xHxSF5B1UNE/FPqAHwB2B/4RkTclnNYOyyNxwdYkD7UKssU/AXZKpRF8RSwpGht+FVekrQnaUSSpAlkM8iL4jLgx8CU9PiUVHZsXgG5eQeoNH1UPe4NLI6IkTmGVVepo7N67Z0/5BjODpPU2fozERHv7eT5lpHmHHwLuJMt16a5YJsntZD04f19YBTZ4muDgCkR8VCugdWJpAcjYkxXZY1U6pq+pJlkywzvLOmFSjHwKk2yc/2OkvQZ4Dzgj2RfoUVWq9ovz7h2VES8p5bjJE1LK462qlnAi2RLTBRh0lJ7S4E/JfsWKmA5xWp2/r2kU8i+iUK24NqzOcbjmj6ApL+PiJl5x9ETJD1KNhqi5Se8bI/23+JajaS2iBiXdxw9paO/T6v/zaqlBQF/ABxBVtn6NXB2++bkRip1Tb/KTZJ2jYiX0qfyWOB7ef5h6uh/ySZklVWrT7S7XdJxEXFr3oHUU4l2dlvTbMtEu6bPpmF/hwAHk+1wcynZaJ5Ol39tBemCuoxs7ZbqNuHcFnxqpFavNaZt93Yl+9u9RkGGbEqaRraz2zjgfjYn/ReAuXlOXqonSSvINr65G7gLuCcicu2odtJnc2KQ9A3gdxFxaasni4q0OcWvgMWkYXGwaWetwpP0QEQcmncc1rGudnYrQJ9MpYnnXWRr6n8AeM4duflblzp1Pw68K43e6ZNzTPWyISK+2PVhhXVP3gHsiLSByh2V2qGkNwFHR8RP84yrXjpL+MnZQMsm/TQm/yiypH8IWcf1r3KNyTX9Te2LHwXuj4i70yfz0RFxec6h7TBJs4AnyBbpqm7eaekhmxWSziZrvlpHNpv6UGBGUdrAtzHkrzTfXlr9vUp6naz56u8i4oa84wEn/U2KujtRWke/vYiIlh6yWSHpoTR9fyLZ2vPnAJcVoWkOsv6m9ktKSFpclmUKWr2ZVdIhZHtavBt4C/AocGeeawu5eYdi704UEcPzjqGHVToAP0CW7B8q0tLYZEt+XwBcRDbk73Nk+x2XRUv/LdP/x/8lG0X3LrIZue8mGyySCyf9TKF3J5J0JDCMLWfktnzTVbJQ0q1ke+POTCuIvt7FOa3kc2TfXq4mS4C3kv1/LYtW75NpI9vn4ddkbfnvznsouJt3AEn3RcQ7Ku2HaaW/RUVYqVHSFWTfXh5k80JWUZQhm2kl1DHAYxHxXFrHZXBEPJxvZPWVltF4PSJezDuWeipBn8ygiNjmTm55jE5yTT/Tfneiv6I4uxONA0YWbcEuSQdExG/IEj7AfsVq1clIGg1cTtb0iKTfA9MiYkmugdXPJyPie6lPZhDZJkaXkX2jaXmdJfyk4aOTnPQzM8h2J1pMtlXbz8hqHUWwhGw7yKfzDqTOvkjWD/PPHTwXbLlpRSv7IfDFiPgFgKSjydaFOjLHmOqp6H0yXWn4ey11846k+RFxjKTvRMRX8o6nJ6TVKMcAC9hyyGZTTQ3vKZKObeWlpCujk7oqa1WSLiMbODGcbBx7b+CXEdF0e8v2hDxGJ5W9pr+PpD8FTpB0Fe0+dSNiUT5h1dW5eQeQs+8ALZv0gccknUO2PAhkoz86Gobbqk5jc5/My6lP5hP5htRQDa/plz3pf4OsaWcI0H598kI0EUTEnXnHkLNWbyr4JPC3ZHuqimxd/ZZPimXpk6lBw0cnlbp5p0LSORHxrbzjqCdJv4qId6YFu6r/yIVYsKtWrT65p1paHmTXiHihy4ObnKRLImL6NjbDKdImOE03OqnUSb9S26jaem8LBWneKbVWT/qSfgycTjbcdiGwG3BBRPxjroE1SFH6ZJppxnjZm3f+Gvg0xR8BQpps1q/yOCKezDGcRlqZdwA7aGREvCDpY2Sjyr5ClvxLkfRp/T6ZphudVOqkHxGfTrc1bb3XiiSdQPahti+wBngr8AhwUJ5x7ShJJ3X2fGU99ojo9LgW0EdSH+BE4AcR8ZqkMn09b/WG/qabMV7qpF9r4mhx3wImALen2cbvIduns9V9qJPngqzjswh+SPZt5SHgrrQwYMu36XdDq3/ANd3opLK36V/WydMREZ9sWDA9pLLHqqSHgEMj4nVJCyJifN6xWfelpoHeEbEhPW75TUY606p9Ms3cX1jqmn5E1PSJ2+IX1nOS3ki2Vdt/SloDbMg5prqRtDfwd8C+EfF+SSPJNoLPbRXDnpSW06j++7X0JiM1WJl3ANupaWeMl7qmX6tWrW0ASNoV+CPQC/gY2eiP/yjQJio3kw2J+1oaJbET8ECJ1ptvyU1GStK02qU8RieVuqbfDa3cmfSNtMTE66QaoaTvkI0CKYKBETEvbXdJRGyQtLGrkwqkVWttZemT6UrDRyc56demVS8sgGPZOsG/v4OyVvVS6hwLAEkTgOfzDamhWrJCUmvTagl4GYYm1XIXlqQzyJaIfpuk6rXl+9PiG1O080XgRrL3eQ/Z8rwfyTekhmrpv2XZ+mQ60PAKpdv0ayDpBxHx2bzj6A5JuwG7A39Ptr5QxbqitOdXpHb8/ck+nJdHxGs5h1Q3zTiNv57cJ9P4/sJejXyxZiXpbEkDlLlU0iJJx1Web7WEDxARzwNPAqMj4omqn0Il/GQ82bK8Y4GTJZ2aczz19Mm01s5xbN5k5Px8Q6qrgRExjzRhKQ1FLVOfzMpGv6CbdzKF3L0njcl/SNJbirrswra2gyTbbaoImm4af50Vsk+mmWeMO+lninxh7QMslbQAeKlSWKBNVAq5HWSVppvGX2dF7ZNp2tFJbtOn2Lv3pE1itlKUdfYlXQOcFRFF2w4SKMfG70Xuk2lGTvoU/8JK67WMiIjbJe1CNo1/Xd5x1UNRt4Ns5mn89SbpSGAYVS0PEVGI5rlmHJ1U6qRfhgtL0qfJpoPvERFvkzQC+NeIOCbn0OqiqN9kSrTJSId9MhFxVm5B1VEzjk4qe9Iv/IUl6UGy0S33VabrS1pcliFxRVeATUYeocB9MpLuj4jDq5fLkPRgRIzJK6ZSd+RGxPR02+l6+i1+Ya2PiFcr/dKpplGYCyyNkvgOsBdZm3CptoOk9TcZWQK8GShknwxNODqp1Em/G1r5wrpT0leBnSUdSzZL979yjqme/gH4UEQ8kncgOWn1UWYDgWVpdFlh+mSqNN3opFI379SqVVcyhE2d1KeRTe4RcEtE/Fu+UdWPpHsi4qi848hLK68AC8Xtk6nWbKOTXNOvTSt/Mn4uIr4HbEr0ks5OZUXQJulq4KdsWVMsyyqNLa1Iyb0T49k8OmmspFxHJznpF980oH2C/8sOylrVAOBlsm8yFWVamndl3gHsiKL3yTTjjHE379RA0k/ymC69IySdDHwUeCdwd9VT/YGNEfG+XAKzmpRlkxFJKyhwn0wzjk4qdU2/mdfHqINfk42IGMiWW7atAwox6QxAUj+yPouDgH6V8mj9/Y2bdhp/nT1T1ISfNN3opFLX9FWCjdGLLi3D8BuybzXnkW0J+UhEnJ1rYFYTSd8jS4o/pYB9Ms04Y7zUSb8MStBm+kBEHCrp4Yg4WFIfshFKLT+xDppzGn89baPiVZgKVzOOTnLSp9gXVgnaTBdExHhJd5HNQfg/YEFE7JdzaHXRjNP4rbV5E5XMHOAWYN/0+LfA5/MKps6K3mZ6iaTdgXPIJsEsI5uwVRSF3mREUj9JZ0q6WNK/V37yjqteJJ0k6VFJz0t6QdI6SS/kGVOpO3KrDIyIeZJmQnZhSSrKhVXocewR8aN0906gELX7dppuGn+dXUHWJzORqj6ZXCOqr6abMe6knynyhVXoceyS+gIfZuulec/LK6Y6a7pp/HX2JxExRdLkiJgr6cdk37qLoum+aTvpZwp7YUXEJ/KOoYfdQPYBvZCqbzJFERGLUmdg00zjr7PKe3lO0iiyPplh+YVTd033TdsduUmzrY9RL5LeDswG9o6IUZIOBk6IiG/nHFpdSFoSEaPyjqMnFXyTkU8B1wEHk3VYvxH4RkT8a66B1Ukzjk5y0k+KemFJuhP4MvDDqvW8C5MoJV0CfD8iFucdS08o+iYj1nhu3qE518eoo10iYoG23Od9Q17B1IukxWR/o52AT0h6jOzrc2UewsF5xldHhd74veh9Ms04Y9xJP1PkC+v3kt7G5k7qj9BEU8J3wPF5B9AgTTeNv84K3SdDE45OctLPFPnCOhO4BDhA0u+Ax8n+47W0iHgCNo20Whppo3dJ/YGRwBM5hldPRd9kZEhETMo7iB7UdKOTnPQzhb2wIuIx4H2SdgV6VZJjhaRpETE3n+jqYjZQvYnISx2UtbJz8w6gh/1a0uii9snQhKOTnPQz5+YdQE+LiJe28dTZQCsnfVU3y0XE62kkViEUdZOREvXJtJ8x/kbgG3kG5NE7JdfKW0FCttcB8Euy2j1k6++8JyJOzCumeirqgnmS3trZ85XmO6s/J32Ke2HVogB7rO4FXAi8l6zmOB/4fESsyTWwOinBgnkd9slExH35RlYfzTg6yUmf4l9YnWn1mn5XJM2MiL/PO47tVfSN3yU9AIytNNFJ6gW0tXJFpJqkn7N5dNKm9bwi4p+3eVIPK0zb5w5quvUxGuievAPoYVOAlk36NOE0/jordJ8MTTg6yUsrZ9okXS3p5LQU6kldbaXYKiSdLWmAMpdKWiRp0+JrEfHZPONrAHV9SFOrXjDvQ+mnSHMUHpN0lqQ+6eds4LG8g6qjX0tqqr0P3LxDc66PUS+SHkqbb0wkG7N/DnBZUb4+d6XV+yyKrqh9Mu1GJ40g+yBritFJRfoatd0KvhJlpab7AbJk/5DarclQcC39XptxGn89peQ+dVvPt3CfTNN+G3PzDoXfvWehpFvJkv4taXTE6znH1EjX5B3ADrqCbLb4RLKNYoYA6zo9o1im5B3A9oiIJ9Kw032AP1Q9/gPZ3zM3bt4BJF1Dtj7GR6laHyMizs41sDpIoyHGAI9FxHNps5jBEfFwvpHVR9Frwir4xu9dafXRZc04Osk1/cyfRMQ5wEtpSYIPAk3V+dJdkg5Id8ek2/0kjQXeSrGa9YpeE24/jX83irXJSFdavVa61egkcr7+inTx74imWx+jDr4ITAc6Gg8cZB1nRdB0C1rVWdNN42+wlu6TIY1OYssZ47mOTnLSzxTuwoqI6en2PZ0dJ+nYiLitMVH1iCJ+YG8Sxd/4vSut3idzOtnopK+zeXTS9DwDcpt+ybX6kMaq7fZGA3PIPrDPiYgf5hlXvTTjNP56KnqfTFfyGJ3kmj7Fv7C60Opfn+dHxP8D7iLVhCUNzzekuvImI8XW8BnjTvqZol9YnWn1r3rXsfXa+dcCh+UQS09oumn8dVb0PpmuNLzS5aSfKfqFVThpdNJBwG7tlswYQFUzQQF4k5Fia3ily0k/U/QLqzMr8w5gO+1PNuvxTWTr0VSsAz6dR0D1VMJNRr7O5kEU5+QbUkM1vKZf6o7cZl4fY0d1tWBcUVZplHRERNybdxz1VpZNRiQNj4jHuyorKklfjYi/a+hrljzpF/bC2sYichWFWEwOij/6owSbjGw1ekzSwogoRJ9MM/7/LHXzTiWpb+vCAlo26Rd8EblqRR/9UciN30vUJ9N0/z+9DENmNvBi1ePKhdXyJO2d1tG/OT0eKem0vOOqo8ItodFO003jr5P2fTKVn7EUoE+mStP9/yzCf556KPLuPXOAy4Cvpce/Ba4GLs0roDor+uiPppvGXw8RcQNwQ1H7ZKo03f9P1/QzRd69Z2BEzCMtpxwRG6jaq7MA2o/+WEa2yX1RnA4cCfwOWAW8g5yn8dfZAwVe1hya8P9nUWqzO6rp1seoo5fScsqVpV0nkE1Ea2mSvlj1sNJ/cVG63bXB4fSYAm8yUtF0bd511nQzxks9eqdWrXxhpeWUvw+MApYAg4CPtPp6+pK+me7uDxxOVouCrF34roj4VC6BNVgB1k4q9H4BzTg6yTX92jR8fYx6iYhFkv6ULDkKWB4Rr3VxWtOLiL8FSLuCja0aeXUurb8yY3e0+tpJTdfmXQ/NPDrJSb82rX5hjWfzYnJjJRERl+cbUt28BXi16vGrFCBpdEOrf1Uv6ozcpp0x7qRfm5a9sCRdAbwNeJDNHbgBFCXpXwEskHQ92fv6M2BuviE1VEtWSIreJ9PMo5Oc9GvTkhdWMo5sBmfLfnB1JiJmpTkI70pFn4iIB/KMqcFatSmrf7rtsE8ml4h6xgOSzsQzcltOq15YkHXevhl4Ou9AekpELAIW5R1HT+hqGn+j122plxL1yTTd6CQnfYp7YSUDgWWSFlC1V0BEnJBfSNYNTZc06qzofTJNt1+Ak36myBfWuXkHYDuk6ZJGnRW9T6bpRic56WcKe2FFxJ15x2A7pOmSRj2VoE+m6UYnOelnCnthpTHC3wH2IuuQruwVMCDXwKxWTZc06q2IfTLNPDrJST9T5AvrH4APRURRmqvKpumm8VtNmnZ0kpdhoNi790i6JyKOyjsO2z7NOI3fapdGJ3243V4d1+S5J7dr+pnr2HpTimuBIlxYbZKuBn7KlqN3CrFdYlE18zR+65amG51U6qRfkgtrAPAycFxVWQBO+s2taafxW7c03eikUjfvSJoMnAicwOY2N8gurKsi4td5xGVW0YzT+K170kq3ldFJd+U9OqnUSb+iyBdWM27MbLXz38/qzTtnZYq8e88VZMswTATuBIaQfZOx1uC/n9WVk36myBdW023MbN3iv5/VlZN+psgXVvuJZ7tRkIlnJeG/n9VVqUfvVCnsjFw2Tzw7h80Tz76Rb0jWDUWeOGg5cEcuIOlTZGP1RwNzSBdWRPwwz7isvNpN499UnG4jIi5oZDxWHKWu6Tfz+hj1Iqkv8GE2b5cIQEScl1dMVpOmncZvra3USZ9yXFg3AM8DC6makWvNrUSbjFiDuXmH5lwfo14kLYmIUXnHYdtH0m+AQyJifXrcF3goIg7INzJrVWWv6Vc03foYdfRrSaMjYnHegdh2abpp/NbaXNMHJH0N+HOg+sK6OiL+PtfAdoCkxWTvZSdgBPAYWfNOZT39g3MMz7qh2abxW2tz0k+KdmFJemtnz0fEE42Kxcyah5N+wUmaACxt118xMiLuyzcyM8uDk37BSXqAbPRHpMe9gLb2G3OYWTl4GYbiU1R9skfE67gD36y0nPSL7zFJZ0nqk37OJuvUNbMSctIvvtOBI4HfAauAdwDTc43IzHLjNv2SkzSzlYemmln3uKZvU/IOwMwax0nf1PUhZlYUTvrm9j2zEnHSN9f0zUrESd+8TK9ZiXj0TsFJ6gecBhwE9KuUR8QncwvKzHLjmn7xXQG8GZgI3AkMAdblGpGZ5cY1/YKT9EBEHCrp4Yg4WFIf4JaIeG/esZlZ47mmX3yvpdvnJI0CdqM4G8SYWTd54a3iu0TS7sDXyfYAfiNwTr4hmVle3LxTcJKGR8TjXZWZWTm4eaf4ruug7NqGR2FmTcHNOwUl6QCyYZq7STqp6qkBVA3dNLNycdIvrv2B44E3AR+qKl8HfDqPgMwsf27TLzhJR0TEvXnHYWbNwUm/4Dwj18yquSO3+Dwj18w2cU2/4Dwj18yquaZffJ6Ra2abePRO8XlGrplt4uadgpL0xY6K021ExAWNjMfMmoNr+sXVP93uDxxOVsuHbMz+XblEZGa5c02/4CTdCnw4Italx/2BayJiUr6RmVke3JFbfG8BXq16/CruyDUrLTfvFN8VwAJJ1wMB/BkwN9+QzCwvbt4pAUljgXelh3dFxAN5xmNm+XHSNzMrEbfpm5mViJO+mVmJOOlbqUg6S9Ijkv6zm+cNk/TRnorLrFGc9K1s/gr4QER8rJvnDQO6nfQl9e7uOWY9yUnfSkPSvwL7ATdK+pqkf5d0v6QHJE1OxwyTdLekRennyHT6+cC7JD0o6QuS/lLSD6p+902Sjk73X5R0nqT7gCMknSJpQTr3h5J6p585kpZIWizpCw39x7DSctK30oiI04HVwHuAXYE7IuLw9PgfJe0KrAGOjYixwF8AF6bTZwB3R8SYiPiXLl5qV2BJRLwDeDb9nqMiYgywEfgYMAYYHBGjImI0cFn93qnZtnlylpXVccAJkr6UHvcjm728GviBpDFkCfrt2/G7NwLXpfvHAIcB90sC2Jnsg+W/gP0kfR/4b+DW7XsbZt3jpG9lJbI1iZZvUSidCzwDHEL2TfiVbZy/gS2/Kferuv9KRGysep25ETFzqwCkQ8h2NDsT+HPAW1haj3PzjpXVLcDnlKrfkg5N5bsBT0fE68DHgUpH7Do2r1wKsBIYI6mXpKHA+G28znzgI5L2Sq+zh6S3ShoI9IqI68j2Nxhbv7dmtm2u6VtZfQv4LvBwSvwrgeOBi4HrJE0BfgG8lI5/GNgg6SFgTjr3cWAxsARY1NGLRMQySV8HbpXUi2wnszOBPwKXpTKArb4JmPUEL8NgZlYibt4xMysRJ30zsxJx0jczKxEnfTOzEnHSNzMrESd9M7MScdI3MysRJ30zsxL5/9oeocMO2v19AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def barPlotCount():\n",
    "    \n",
    "    features = ['data_channel_is_lifestyle', 'data_channel_is_entertainment', 'data_channel_is_bus', \n",
    "         'data_channel_is_socmed', 'data_channel_is_tech', 'data_channel_is_world']\n",
    "    new = pd.DataFrame({'features':[], 'samples':[]})\n",
    "    \n",
    "    \n",
    "    for f in features:\n",
    "        new = new.append({'features': f, 'samples': data2[data2[f]==1].shape[0]}, ignore_index=True)\n",
    "    ax = new.plot.bar(x='features', y='samples')\n",
    "    return\n",
    "\n",
    "barPlotCount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Our data still needs some value scaling/normalization, but further preprocessing will be model-dependant and will be done \n",
    "#along the implementation of the model itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize share columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "trans = preprocessing.KBinsDiscretizer(n_bins=6, encode='ordinal', strategy='quantile')\n",
    "target_y = np.array(data[59]).reshape(-1, 1)\n",
    "y1 = []\n",
    "for y in target_y:\n",
    "    try:\n",
    "        y1.append(float(y[0]))\n",
    "    except:\n",
    "        print(y)\n",
    "        y1.append(1100)\n",
    "        \n",
    "y_target = np.array(y1).reshape(-1, 1)\n",
    "y_discretized = trans.fit_transform(y_target)\n",
    "plt.hist(y_discretized)\n",
    "plt.show\n",
    "y = y_discretized\n",
    "X = data.drop([59], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Sampling share columns using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23946, 59)\n",
      "(23946,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAATQklEQVR4nO3dcaid9Z3n8fdnomulrVTxKpncsJGSGVaFSddLVhCWbu2OWVsmFlaIMOofLikSwbIDg5l/pv0j0D+m7SCsQjoV4063EmiLodPOTiZTKYI1vTqpMaYZw+jobYK501Kq/2RJ/O4f5ycc4sm95+bmnjvm937B4TzP9/n9nuf3KPnch995znlSVUiS+vA7qz0ASdLkGPqS1BFDX5I6YuhLUkcMfUnqyGWrPYDFXHvttbVhw4bVHoYkfai8+OKL/1pVU+fW/82H/oYNG5idnV3tYUjSh0qSfxlVd3pHkjpi6EtSRwx9SeqIoS9JHTH0JakjY4d+kjVJ/jHJD9r6NUn2J3mtvV891HZnkuNJjiW5Y6h+S5LDbdujSXJxT0eStJClXOk/DBwdWn8EOFBVG4EDbZ0kNwLbgJuALcBjSda0Po8D24GN7bVlWaOXJC3JWKGfZBr4HPBXQ+WtwJ62vAe4a6j+dFWdrqrXgePA5iRrgauq6vka/J7zU0N9JEkTMO6V/l8Cfwq8N1S7vqpOArT361p9HfDWULu5VlvXls+tf0CS7Ulmk8zOz8+POURJ0mIW/UZuks8Dp6rqxSSfHmOfo+bpa4H6B4tVu4HdADMzMxf8lJcNj/zNhXZdlje++rlVOS54zpPkOU/Oap3valqp/9bj/AzDbcAfJbkT+AhwVZK/Bt5OsraqTrapm1Ot/Rywfqj/NHCi1adH1CVJE7Lo9E5V7ayq6arawOAD2n+oqj8G9gH3t2b3A8+05X3AtiRXJLmBwQe2B9sU0DtJbm137dw31EeSNAHL+cG1rwJ7kzwAvAncDVBVR5LsBV4FzgA7qups6/Mg8CRwJfCj9pIkTciSQr+qngWebcu/Am4/T7tdwK4R9Vng5qUOUpJ0cfiNXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerIoqGf5CNJDib5eZIjSb7S6l9O8sskh9rrzqE+O5McT3IsyR1D9VuSHG7bHm3PypUkTcg4j0s8DXymqt5NcjnwXJL3n237jar6i+HGSW5k8AD1m4DfBf4+ye+15+Q+DmwHfgr8ENiCz8mVpIlZ9Eq/Bt5tq5e3Vy3QZSvwdFWdrqrXgePA5iRrgauq6vmqKuAp4K5ljV6StCRjzeknWZPkEHAK2F9VL7RNDyV5OckTSa5utXXAW0Pd51ptXVs+tz7qeNuTzCaZnZ+fH/9sJEkLGiv0q+psVW0Cphlctd/MYKrmk8Am4CTwtdZ81Dx9LVAfdbzdVTVTVTNTU1PjDFGSNIYl3b1TVb8BngW2VNXb7Y/Be8A3gc2t2RywfqjbNHCi1adH1CVJEzLO3TtTST7Rlq8EPgv8os3Rv+8LwCtteR+wLckVSW4ANgIHq+ok8E6SW9tdO/cBz1y8U5EkLWacu3fWAnuSrGHwR2JvVf0gyf9OsonBFM0bwBcBqupIkr3Aq8AZYEe7cwfgQeBJ4EoGd+14544kTdCioV9VLwOfGlG/d4E+u4BdI+qzwM1LHKMk6SLxG7mS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkXGekfuRJAeT/DzJkSRfafVrkuxP8lp7v3qoz84kx5McS3LHUP2WJIfbtkfbs3IlSRMyzpX+aeAzVfUHwCZgS5JbgUeAA1W1ETjQ1klyI7ANuAnYAjzWnq8L8DiwncHD0je27ZKkCVk09Gvg3bZ6eXsVsBXY0+p7gLva8lbg6ao6XVWvA8eBzUnWAldV1fNVVcBTQ30kSRMw1px+kjVJDgGngP1V9QJwfVWdBGjv17Xm64C3hrrPtdq6tnxufdTxtieZTTI7Pz+/hNORJC1krNCvqrNVtQmYZnDVfvMCzUfN09cC9VHH211VM1U1MzU1Nc4QJUljWNLdO1X1G+BZBnPxb7cpG9r7qdZsDlg/1G0aONHq0yPqkqQJGefunakkn2jLVwKfBX4B7APub83uB55py/uAbUmuSHIDgw9sD7YpoHeS3Nru2rlvqI8kaQIuG6PNWmBPuwPnd4C9VfWDJM8De5M8ALwJ3A1QVUeS7AVeBc4AO6rqbNvXg8CTwJXAj9pLkjQhi4Z+Vb0MfGpE/VfA7efpswvYNaI+Cyz0eYAkaQX5jVxJ6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyDjPyF2f5MdJjiY5kuThVv9ykl8mOdRedw712ZnkeJJjSe4Yqt+S5HDb9mh7Vq4kaULGeUbuGeBPquqlJB8HXkyyv237RlX9xXDjJDcC24CbgN8F/j7J77Xn5D4ObAd+CvwQ2ILPyZWkiVn0Sr+qTlbVS235HeAosG6BLluBp6vqdFW9DhwHNidZC1xVVc9XVQFPAXct9wQkSeNb0px+kg0MHpL+Qis9lOTlJE8kubrV1gFvDXWba7V1bfnc+qjjbE8ym2R2fn5+KUOUJC1g7NBP8jHgu8CXquq3DKZqPglsAk4CX3u/6YjutUD9g8Wq3VU1U1UzU1NT4w5RkrSIsUI/yeUMAv/bVfU9gKp6u6rOVtV7wDeBza35HLB+qPs0cKLVp0fUJUkTMs7dOwG+BRytqq8P1dcONfsC8Epb3gdsS3JFkhuAjcDBqjoJvJPk1rbP+4BnLtJ5SJLGMM7dO7cB9wKHkxxqtT8D7kmyicEUzRvAFwGq6kiSvcCrDO782dHu3AF4EHgSuJLBXTveuSNJE7Ro6FfVc4yej//hAn12AbtG1GeBm5cyQEnSxeM3ciWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakj4zwjd32SHyc5muRIkodb/Zok+5O81t6vHuqzM8nxJMeS3DFUvyXJ4bbt0fasXEnShIxzpX8G+JOq+g/ArcCOJDcCjwAHqmojcKCt07ZtA24CtgCPJVnT9vU4sJ3Bw9I3tu2SpAlZNPSr6mRVvdSW3wGOAuuArcCe1mwPcFdb3go8XVWnq+p14DiwOcla4Kqqer6qCnhqqI8kaQKWNKefZAPwKeAF4PqqOgmDPwzAda3ZOuCtoW5zrbauLZ9bH3Wc7Ulmk8zOz88vZYiSpAWMHfpJPgZ8F/hSVf12oaYjarVA/YPFqt1VNVNVM1NTU+MOUZK0iLFCP8nlDAL/21X1vVZ+u03Z0N5PtfocsH6o+zRwotWnR9QlSRMyzt07Ab4FHK2qrw9t2gfc35bvB54Zqm9LckWSGxh8YHuwTQG9k+TWts/7hvpIkibgsjHa3AbcCxxOcqjV/gz4KrA3yQPAm8DdAFV1JMle4FUGd/7sqKqzrd+DwJPAlcCP2kuSNCGLhn5VPcfo+XiA28/TZxewa0R9Frh5KQOUJF08fiNXkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOjLOM3KfSHIqyStDtS8n+WWSQ+1159C2nUmOJzmW5I6h+i1JDrdtj7bn5EqSJmicK/0ngS0j6t+oqk3t9UOAJDcC24CbWp/Hkqxp7R8HtjN4UPrG8+xTkrSCFg39qvoJ8Osx97cVeLqqTlfV68BxYHOStcBVVfV8VRXwFHDXBY5ZknSBljOn/1CSl9v0z9Wttg54a6jNXKuta8vn1kdKsj3JbJLZ+fn5ZQxRkjTsQkP/ceCTwCbgJPC1Vh81T18L1Eeqqt1VNVNVM1NTUxc4REnSuS4o9Kvq7ao6W1XvAd8ENrdNc8D6oabTwIlWnx5RlyRN0AWFfpujf98XgPfv7NkHbEtyRZIbGHxge7CqTgLvJLm13bVzH/DMMsYtSboAly3WIMl3gE8D1yaZA/4c+HSSTQymaN4AvghQVUeS7AVeBc4AO6rqbNvVgwzuBLoS+FF7SZImaNHQr6p7RpS/tUD7XcCuEfVZ4OYljU6SdFH5jVxJ6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyKKhn+SJJKeSvDJUuybJ/iSvtferh7btTHI8ybEkdwzVb0lyuG17tD0rV5I0QeNc6T8JbDmn9ghwoKo2AgfaOkluBLYBN7U+jyVZ0/o8Dmxn8LD0jSP2KUlaYYuGflX9BPj1OeWtwJ62vAe4a6j+dFWdrqrXgePA5iRrgauq6vmqKuCpoT6SpAm50Dn966vqJEB7v67V1wFvDbWba7V1bfnc+khJtieZTTI7Pz9/gUOUJJ3rYn+QO2qevhaoj1RVu6tqpqpmpqamLtrgJKl3Fxr6b7cpG9r7qVafA9YPtZsGTrT69Ii6JGmCLjT09wH3t+X7gWeG6tuSXJHkBgYf2B5sU0DvJLm13bVz31AfSdKEXLZYgyTfAT4NXJtkDvhz4KvA3iQPAG8CdwNU1ZEke4FXgTPAjqo623b1IIM7ga4EftRekqQJWjT0q+qe82y6/TztdwG7RtRngZuXNDpJ0kXlN3IlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI8sK/SRvJDmc5FCS2Va7Jsn+JK+196uH2u9McjzJsSR3LHfwkqSluRhX+v+lqjZV1UxbfwQ4UFUbgQNtnSQ3AtuAm4AtwGNJ1lyE40uSxrQS0ztbgT1teQ9w11D96ao6XVWvA8eBzStwfEnSeSw39Av4uyQvJtneatdX1UmA9n5dq68D3hrqO9dqH5Bke5LZJLPz8/PLHKIk6X2XLbP/bVV1Isl1wP4kv1igbUbUalTDqtoN7AaYmZkZ2UaStHTLutKvqhPt/RTwfQbTNW8nWQvQ3k+15nPA+qHu08CJ5RxfkrQ0Fxz6ST6a5OPvLwN/CLwC7APub83uB55py/uAbUmuSHIDsBE4eKHHlyQt3XKmd64Hvp/k/f38n6r62yQ/A/YmeQB4E7gboKqOJNkLvAqcAXZU1dlljV6StCQXHPpV9c/AH4yo/wq4/Tx9dgG7LvSYkqTl8Ru5ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1JGJh36SLUmOJTme5JFJH1+SejbR0E+yBvhfwH8DbgTuSXLjJMcgST2b9JX+ZuB4Vf1zVf0/4Glg64THIEndSlVN7mDJfwe2VNX/aOv3Av+pqh46p912YHtb/X3g2AUe8lrgXy+w74eV59yH3s65t/OF5Z/zv6+qqXOLly1jhxciI2of+KtTVbuB3cs+WDJbVTPL3c+Hiefch97OubfzhZU750lP78wB64fWp4ETEx6DJHVr0qH/M2BjkhuS/DtgG7BvwmOQpG5NdHqnqs4keQj4v8Aa4ImqOrKCh1z2FNGHkOfch97OubfzhRU654l+kCtJWl1+I1eSOmLoS1JHLsnQ7/GnHpI8keRUkldWeyyTkGR9kh8nOZrkSJKHV3tMKy3JR5IcTPLzds5fWe0xTUqSNUn+MckPVnssk5DkjSSHkxxKMntR932pzem3n3r4J+C/MrhF9GfAPVX16qoObIUl+c/Au8BTVXXzao9npSVZC6ytqpeSfBx4EbjrUv7/nCTAR6vq3SSXA88BD1fVT1d5aCsuyf8EZoCrqurzqz2elZbkDWCmqi76F9IuxSv9Ln/qoap+Avx6tccxKVV1sqpeasvvAEeBdas7qpVVA++21cvb69K6ahshyTTwOeCvVnssl4JLMfTXAW8Nrc9xiYdB75JsAD4FvLDKQ1lxbZrjEHAK2F9Vl/w5A38J/Cnw3iqPY5IK+LskL7afpbloLsXQH+unHnRpSPIx4LvAl6rqt6s9npVWVWerahODb7NvTnJJT+Ul+TxwqqpeXO2xTNhtVfUfGfwi8Y42fXtRXIqh7089dKLNa38X+HZVfW+1xzNJVfUb4Flgy+qOZMXdBvxRm+N+GvhMkr9e3SGtvKo60d5PAd9nMG19UVyKoe9PPXSgfaj5LeBoVX19tcczCUmmknyiLV8JfBb4xaoOaoVV1c6qmq6qDQz+Lf9DVf3xKg9rRSX5aLs5gSQfBf4QuGh35V1yoV9VZ4D3f+rhKLB3hX/q4d+EJN8Bngd+P8lckgdWe0wr7DbgXgZXfofa687VHtQKWwv8OMnLDC5u9ldVF7cwduZ64LkkPwcOAn9TVX97sXZ+yd2yKUk6v0vuSl+SdH6GviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerI/wcZXMCYjVFS1QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn import over_sampling\n",
    "over = over_sampling.SMOTE()\n",
    "X = data.drop([59], axis=1) \n",
    "#remove non-numerical values to perform K-Means value SMOTE only on measurable ones.\n",
    "#column 60 is also removed, as it contains the original score.\n",
    "X.head()\n",
    "y  = y_discretized\n",
    "#SMOTE is apply iteratively to make sure that all classes have the same number of samples (with only one iteration there is still disparity)\n",
    "X, y = over.fit_resample(X, y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "plt.hist(y)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature importance analysis  (up to 1 of 11.2 points)\n",
    "\n",
    "Perform feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23946, 59) (23946,)\n"
     ]
    }
   ],
   "source": [
    "#feature importance analysis is useful to find the best feature for train a model.\n",
    "# these techiques returns\n",
    "# Finally we will make a features selections in which will going to delete the features that doesen't fit with the model\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "#X = data.drop(data[59],axis=1)\n",
    "#X = data\n",
    "#y = y_discretized\n",
    "#X = data.drop([59], axis=1) \n",
    "print(X.shape,y.shape)\n",
    "#print(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.03140\n",
      "Feature: 1, Score: 0.01984\n",
      "Feature: 2, Score: 0.02601\n",
      "Feature: 3, Score: 0.02610\n",
      "Feature: 4, Score: 0.00016\n",
      "Feature: 5, Score: 0.02635\n",
      "Feature: 6, Score: 0.02208\n",
      "Feature: 7, Score: 0.01573\n",
      "Feature: 8, Score: 0.01409\n",
      "Feature: 9, Score: 0.00846\n",
      "Feature: 10, Score: 0.02773\n",
      "Feature: 11, Score: 0.01344\n",
      "Feature: 12, Score: 0.00000\n",
      "Feature: 13, Score: 0.00346\n",
      "Feature: 14, Score: 0.00229\n",
      "Feature: 15, Score: 0.00000\n",
      "Feature: 16, Score: 0.00372\n",
      "Feature: 17, Score: 0.00255\n",
      "Feature: 18, Score: 0.00522\n",
      "Feature: 19, Score: 0.02710\n",
      "Feature: 20, Score: 0.02856\n",
      "Feature: 21, Score: 0.01786\n",
      "Feature: 22, Score: 0.00540\n",
      "Feature: 23, Score: 0.02887\n",
      "Feature: 24, Score: 0.02047\n",
      "Feature: 25, Score: 0.03257\n",
      "Feature: 26, Score: 0.03543\n",
      "Feature: 27, Score: 0.02371\n",
      "Feature: 28, Score: 0.02223\n",
      "Feature: 29, Score: 0.02409\n",
      "Feature: 30, Score: 0.00455\n",
      "Feature: 31, Score: 0.00458\n",
      "Feature: 32, Score: 0.00484\n",
      "Feature: 33, Score: 0.00462\n",
      "Feature: 34, Score: 0.00430\n",
      "Feature: 35, Score: 0.00000\n",
      "Feature: 36, Score: 0.00000\n",
      "Feature: 37, Score: 0.00000\n",
      "Feature: 38, Score: 0.02813\n",
      "Feature: 39, Score: 0.02738\n",
      "Feature: 40, Score: 0.02883\n",
      "Feature: 41, Score: 0.02767\n",
      "Feature: 42, Score: 0.02955\n",
      "Feature: 43, Score: 0.02762\n",
      "Feature: 44, Score: 0.02484\n",
      "Feature: 45, Score: 0.02624\n",
      "Feature: 46, Score: 0.02485\n",
      "Feature: 47, Score: 0.02172\n",
      "Feature: 48, Score: 0.02219\n",
      "Feature: 49, Score: 0.02669\n",
      "Feature: 50, Score: 0.01470\n",
      "Feature: 51, Score: 0.01434\n",
      "Feature: 52, Score: 0.02492\n",
      "Feature: 53, Score: 0.01884\n",
      "Feature: 54, Score: 0.01656\n",
      "Feature: 55, Score: 0.01474\n",
      "Feature: 56, Score: 0.01525\n",
      "Feature: 57, Score: 0.01373\n",
      "Feature: 58, Score: 0.01336\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUjUlEQVR4nO3df4ydV53f8fcHkwi6bOWkMdSyTZ2iEV0LLU5kOVlRVTSQyvZWNUhLlayaZLNZmai2BBJSa7ZSC9p/UsSPEimy5SwuiYpwo4U2VtbbNHIXrZCaYCdrjJ3gZpq6ZBI3nqUkLI1E1vDtH/fxcrmMPc/8wDN3zvslXd37nOecO+ck4+dzz3me+0yqCklSe9601B2QJC0NA0CSGmUASFKjDABJapQBIEmNevNSd2Aurrvuutq4ceNSd0OSxsrTTz/9F1W1ZrR8rAJg48aNHD9+fKm7IUljJcn/nqncJSBJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSo3oFQJJtSc4kmUyyd4b9SXJ/t/9kkhu78rck+VaSbyc5neTTQ20+leSlJCe6x47FG5YkaTazfhM4ySrgAeBWYAo4luRwVT07VG07MNE9bgL2dc8/Bm6pqh8luQr4ZpI/qaonu3ZfqKrPLt5wpLnbuPePf2777H2/uUQ9ka6sPjOArcBkVb1QVW8Ah4CdI3V2Ag/XwJPA6iRru+0fdXWu6h7+CTJJWgb6BMA64MWh7amurFedJKuSnADOA09U1VND9fZ0S0YHk1wz0w9PsivJ8STHp6ene3RXktRHnwDIDGWjn+IvWaeqflJVm4H1wNYk7+n27wPeBWwGzgGfm+mHV9WBqtpSVVvWrPmFm9lJkuapTwBMARuGttcDL8+1TlW9CnwD2NZtv9KFw0+BBxksNUmSrpA+AXAMmEhyfZKrgduAwyN1DgN3dlcD3Qy8VlXnkqxJshogyVuBDwLf7bbXDrX/MHBqYUORJM3FrFcBVdWFJHuAx4FVwMGqOp3k3m7/fuAIsAOYBF4H7u6arwUe6q4kehPwSFU91u37TJLNDJaKzgIfXaxBSZJm1+sPwlTVEQYH+eGy/UOvC9g9Q7uTwA2XeM875tRTSdKi8pvAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEb1+iKYtFJ473/pZ5wBSFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEY1cxmol/9J0s9zBiBJjTIAJKlRBoAkNcoAkKRG9QqAJNuSnEkymWTvDPuT5P5u/8kkN3blb0nyrSTfTnI6yaeH2lyb5Ikkz3fP1yzesCRJs5k1AJKsAh4AtgObgNuTbBqpth2Y6B67gH1d+Y+BW6rqvcBmYFuSm7t9e4GjVTUBHO22JUlXSJ8ZwFZgsqpeqKo3gEPAzpE6O4GHa+BJYHWStd32j7o6V3WPGmrzUPf6IeBDCxiHJGmO+gTAOuDFoe2prqxXnSSrkpwAzgNPVNVTXZ13VNU5gO757TP98CS7khxPcnx6erpHdyVJffQJgMxQVn3rVNVPqmozsB7YmuQ9c+lgVR2oqi1VtWXNmjVzaSpJuow+ATAFbBjaXg+8PNc6VfUq8A1gW1f0SpK1AN3z+b6dliQtXJ9bQRwDJpJcD7wE3Ab89kidw8CeJIeAm4DXqupckjXAX1XVq0neCnwQ+LdDbe4C7uueH13waCQtKW+5Ml5mDYCqupBkD/A4sAo4WFWnk9zb7d8PHAF2AJPA68DdXfO1wEPdlURvAh6pqse6ffcBjyS5B/ge8JHFG5YkaTa9bgZXVUcYHOSHy/YPvS5g9wztTgI3XOI9vw98YC6dlSQtHr8JLEmNauZ20Fq5XHf+5fK/78rlDECSGuUMQMuCnzKlK88ZgCQ1yhmA1ChnXXIGIEmNMgAkqVEuAY0pp+9aSqO/f3Dp30F/V5cvZwCS1ChnAJL+mp/W22IA6IrzICMtDy4BSVKjDABJapRLQJJWLJcbL88ZgCQ1yhnAIvMThzQ//tu58gyAnvzllLTSGAAz8GAvqQWeA5CkRhkAktSoXgGQZFuSM0kmk+ydYX+S3N/tP5nkxq58Q5I/TfJcktNJPjbU5lNJXkpyonvsWLxhSZJmM+s5gCSrgAeAW4Ep4FiSw1X17FC17cBE97gJ2Nc9XwA+UVXPJPlV4OkkTwy1/UJVfXbxhiNJ6qvPDGArMFlVL1TVG8AhYOdInZ3AwzXwJLA6ydqqOldVzwBU1V8CzwHrFrH/kqR56nMV0DrgxaHtKQaf7mersw44d7EgyUbgBuCpoXp7ktwJHGcwU/jB6A9PsgvYBfDOd76zR3claeFauBqwTwBkhrKaS50kbwO+Bny8qn7YFe8D/qCr9wfA54Df/YU3qToAHADYsmXL6M+VNIb6HlxbOAgvpT5LQFPAhqHt9cDLfeskuYrBwf8rVfX1ixWq6pWq+klV/RR4kMFSkyTpCukTAMeAiSTXJ7kauA04PFLnMHBndzXQzcBrVXUuSYAvAc9V1eeHGyRZO7T5YeDUvEchSZqzWZeAqupCkj3A48Aq4GBVnU5yb7d/P3AE2AFMAq8Dd3fN3wfcAXwnyYmu7Per6gjwmSSbGSwBnQU+ukhjkiT10OtWEN0B+8hI2f6h1wXsnqHdN5n5/ABVdceceiotQ65Ra5z5TWBJapQ3g5Nm4Cd7tcAZgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRfhFM6skvhy1v/v+ZOwPgCvAXU9Jy5BKQJDXKGYCksTI6owZn1fPlDECSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1qtdloEm2AV8EVgF/WFX3jexPt38H8DrwO1X1TJINwMPA3wZ+Chyoqi92ba4F/iOwETgL/NOq+sEijEnyUkGph1lnAElWAQ8A24FNwO1JNo1U2w5MdI9dwL6u/ALwiar6NeBmYPdQ273A0aqaAI5225KkK6TPEtBWYLKqXqiqN4BDwM6ROjuBh2vgSWB1krVVda6qngGoqr8EngPWDbV5qHv9EPChhQ1FkjQXfQJgHfDi0PYUPzuI966TZCNwA/BUV/SOqjoH0D2/faYfnmRXkuNJjk9PT/foriSpjz7nADJDWc2lTpK3AV8DPl5VP+zfPaiqA8ABgC1btoz+3AXxJm2SWtZnBjAFbBjaXg+83LdOkqsYHPy/UlVfH6rzSpK1XZ21wPm5dV2StBB9AuAYMJHk+iRXA7cBh0fqHAbuzMDNwGtVda67OuhLwHNV9fkZ2tzVvb4LeHTeo5AkzdmsS0BVdSHJHuBxBpeBHqyq00nu7fbvB44wuAR0ksFloHd3zd8H3AF8J8mJruz3q+oIcB/wSJJ7gO8BH1m0UUmSZtXrewDdAfvISNn+odcF7J6h3TeZ+fwAVfV94ANz6awkafH4TWBJapR/EEaS5mAlXT3oDECSGmUASFKjDABJapQBIEmN8iSwlq2VdLJNWo6cAUhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1Ci/ByBJCzSu31lxBiBJjTIAJKlRBoAkNcoAkKRGeRJ4iYyeNILxOXEkaWXoNQNIsi3JmSSTSfbOsD9J7u/2n0xy49C+g0nOJzk10uZTSV5KcqJ77Fj4cCRJfc0aAElWAQ8A24FNwO1JNo1U2w5MdI9dwL6hfV8Gtl3i7b9QVZu7x5E59l2StAB9loC2ApNV9QJAkkPATuDZoTo7gYerqoAnk6xOsraqzlXVnyXZuNgdl6Rxs9y+L9BnCWgd8OLQ9lRXNtc6M9nTLRkdTHLNTBWS7EpyPMnx6enpHm8pSeqjTwBkhrKaR51R+4B3AZuBc8DnZqpUVQeqaktVbVmzZs0sbylJ6qvPEtAUsGFoez3w8jzq/JyqeuXi6yQPAo/16IskLciVWoZZbss9M+kzAzgGTCS5PsnVwG3A4ZE6h4E7u6uBbgZeq6pzl3vTJGuHNj8MnLpUXUnS4pt1BlBVF5LsAR4HVgEHq+p0knu7/fuBI8AOYBJ4Hbj7YvskXwXeD1yXZAr4N1X1JeAzSTYzWCo6C3x08YYlSf2Nw6f1X4ZeXwTrLtE8MlK2f+h1Absv0fb2S5Tf0b+b7Wj1F1HSleetICSpUQaAJDXKewFJ0hJaymVfA2AF8fyBpLkwACRpmblSdwv2HIAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmN6hUASbYlOZNkMsneGfYnyf3d/pNJbhzadzDJ+SSnRtpcm+SJJM93z9csfDiSpL5mDYAkq4AHgO3AJuD2JJtGqm0HJrrHLmDf0L4vA9tmeOu9wNGqmgCOdtuSpCukzwxgKzBZVS9U1RvAIWDnSJ2dwMM18CSwOslagKr6M+D/zvC+O4GHutcPAR+aR/8lSfPUJwDWAS8ObU91ZXOtM+odVXUOoHt++0yVkuxKcjzJ8enp6R7dlST10ScAMkNZzaPOvFTVgaraUlVb1qxZsxhvKUmiXwBMARuGttcDL8+jzqhXLi4Tdc/ne/RFkrRI+gTAMWAiyfVJrgZuAw6P1DkM3NldDXQz8NrF5Z3LOAzc1b2+C3h0Dv2WJC3QrAFQVReAPcDjwHPAI1V1Osm9Se7tqh0BXgAmgQeBf36xfZKvAv8deHeSqST3dLvuA25N8jxwa7ctSbpC3tynUlUdYXCQHy7bP/S6gN2XaHv7Jcq/D3ygd08lSYvKbwJLUqMMAElqlAEgSY3qdQ5AS2vj3j/+ue2z9/3mEvVE0kriDECSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1KheAZBkW5IzSSaT7J1hf5Lc3+0/meTG2dom+VSSl5Kc6B47FmdIkqQ+Zv2LYElWAQ8AtwJTwLEkh6vq2aFq24GJ7nETsA+4qUfbL1TVZxdtNFrx/Oto0uLp8ychtwKTVfUCQJJDwE5gOAB2Ag9XVQFPJlmdZC2wsUdbrVAerKXlrc8S0DrgxaHtqa6sT53Z2u7plowOJrmmd68lSQvWJwAyQ1n1rHO5tvuAdwGbgXPA52b84cmuJMeTHJ+enu7RXUlSH32WgKaADUPb64GXe9a5+lJtq+qVi4VJHgQem+mHV9UB4ADAli1bRoNHGmsuk2kp9QmAY8BEkuuBl4DbgN8eqXOYwXLOIQYngV+rqnNJpi/VNsnaqjrXtf8wcGrBo5GWgdGDOgwO7H0P9oaCrpRZA6CqLiTZAzwOrAIOVtXpJPd2+/cDR4AdwCTwOnD35dp2b/2ZJJsZLAmdBT66iOOSJM2izwyAqjrC4CA/XLZ/6HUBu/u27crvmFNPpYY5K9Avg98ElqRGGQCS1KheS0DSMJcjpJXBGYAkNcoZQIMudZmipLY4A5CkRhkAktQoA0CSGmUASFKjPAmsReGlodL4MQD01zyIS21xCUiSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSo3oFQJJtSc4kmUyyd4b9SXJ/t/9kkhtna5vk2iRPJHm+e75mcYYkSepj1gBIsgp4ANgObAJuT7JppNp2YKJ77AL29Wi7FzhaVRPA0W5bknSF9LkZ3FZgsqpeAEhyCNgJPDtUZyfwcFUV8GSS1UnWAhsv03Yn8P6u/UPAN4B/ucDxXFHePE3SOMvgmH2ZCslvAduq6ve67TuAm6pqz1Cdx4D7quqb3fZRBgfzjZdqm+TVqlo99B4/qKpfWAZKsovBrALg3cCZeY71ouuAv1jgeywnjmd5W0njWUljgbbG83eqas1oYZ8ZQGYoG02NS9Xp0/ayquoAcGAubS4nyfGq2rJY77fUHM/ytpLGs5LGAo4H+p0EngI2DG2vB17uWedybV/plonons/377YkaaH6BMAxYCLJ9UmuBm4DDo/UOQzc2V0NdDPwWlWdm6XtYeCu7vVdwKMLHIskaQ5mXQKqqgtJ9gCPA6uAg1V1Osm93f79wBFgBzAJvA7cfbm23VvfBzyS5B7ge8BHFnVkl7Zoy0nLhONZ3lbSeFbSWMDxzH4SWJK0MvlNYElqlAEgSY1qKgBmu6XFcpfkYJLzSU4NlY3lLTWSbEjyp0meS3I6yce68nEdz1uSfCvJt7vxfLorH8vxwOCb/En+vPuez7iP5WyS7yQ5keR4VzbO41md5I+SfLf7N/Qb8xlPMwHQ85YWy92XgW0jZeN6S40LwCeq6teAm4Hd3f+PcR3Pj4Fbquq9wGZgW3dF3LiOB+BjwHND2+M8FoB/WFWbh66VH+fxfBH4L1X194D3Mvj/NPfxVFUTD+A3gMeHtj8JfHKp+zWPcWwETg1tnwHWdq/XAmeWuo/zHNejwK0rYTzA3wCeAW4a1/Ew+M7OUeAW4LGubCzH0vX3LHDdSNlYjgf4m8D/oruIZyHjaWYGAKwDXhzanurKxt07avCdC7rnty9xf+YsyUbgBuApxng83ZLJCQZfanyiqsZ5PP8O+BfAT4fKxnUsMLgDwX9N8nR3exkY3/H8XWAa+PfdEt0fJvkV5jGelgJgwbel0OJL8jbga8DHq+qHS92fhaiqn1TVZgafnrcmec8Sd2lekvxj4HxVPb3UfVlE76uqGxksAe9O8g+WukML8GbgRmBfVd0A/D/muXzVUgD0uaXFOBrbW2okuYrBwf8rVfX1rnhsx3NRVb3K4O622xjP8bwP+CdJzgKHgFuS/AfGcywAVNXL3fN54D8xuMvxuI5nCpjqZpgAf8QgEOY8npYCoM8tLcbRWN5SI0mALwHPVdXnh3aN63jWJFndvX4r8EHgu4zheKrqk1W1vqo2Mvh38t+q6p8xhmMBSPIrSX714mvgHwGnGNPxVNX/AV5M8u6u6AMMbrE/9/Es9QmNK3zyZAfwP4D/Cfyrpe7PPPr/VeAc8FcMPgXcA/wtBifrnu+er13qfvYcy99nsAR3EjjRPXaM8Xh+HfjzbjyngH/dlY/leIbG9X5+dhJ4LMfCYM38293j9MV/++M6nq7vm4Hj3e/bfwaumc94vBWEJDWqpSUgSdIQA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ16v8DUnHItyW7xj0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03140193 0.01984279 0.02600848 0.02609663 0.0001577  0.02634774\n",
      " 0.02207676 0.01573288 0.01409289 0.00845773 0.02773447 0.01344481\n",
      " 0.         0.00345808 0.00229101 0.         0.00371992 0.00254565\n",
      " 0.00522319 0.02709795 0.02856054 0.01785931 0.00539657 0.0288668\n",
      " 0.02047011 0.0325711  0.03543345 0.02370972 0.0222322  0.02408566\n",
      " 0.00455248 0.0045751  0.00483968 0.00462207 0.00430387 0.\n",
      " 0.         0.         0.02813481 0.02738445 0.02883447 0.02767017\n",
      " 0.02955218 0.0276217  0.02484168 0.02623926 0.02485217 0.02172239\n",
      " 0.02219391 0.02668589 0.01470497 0.01433726 0.02492064 0.01884368\n",
      " 0.01656372 0.01473922 0.01525002 0.01373461 0.01336354]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "# fit the model\n",
    "model.fit(X, y.ravel())\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.04363\n",
      "Feature: 1, Score: 0.03537\n",
      "Feature: 2, Score: 0.03463\n",
      "Feature: 3, Score: 0.03546\n",
      "Feature: 4, Score: 0.02994\n",
      "Feature: 5, Score: 0.03669\n",
      "Feature: 6, Score: 0.03688\n",
      "Feature: 7, Score: 0.03891\n",
      "Feature: 8, Score: 0.03999\n",
      "Feature: 9, Score: 0.02900\n",
      "Feature: 10, Score: 0.04302\n",
      "Feature: 11, Score: 0.04702\n",
      "Feature: 12, Score: 0.03073\n",
      "Feature: 13, Score: 0.02841\n",
      "Feature: 14, Score: 0.03096\n",
      "Feature: 15, Score: 0.03758\n",
      "Feature: 16, Score: 0.03821\n",
      "Feature: 17, Score: 0.03875\n",
      "Feature: 18, Score: 0.03783\n",
      "Feature: 19, Score: 0.03970\n",
      "Feature: 20, Score: 0.03716\n",
      "Feature: 21, Score: 0.03395\n",
      "Feature: 22, Score: 0.03565\n",
      "Feature: 23, Score: 0.03312\n",
      "Feature: 24, Score: 0.02781\n",
      "Feature: 25, Score: 0.02818\n",
      "Feature: 26, Score: 0.03586\n",
      "Feature: 27, Score: 0.03555\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04363202 0.03536626 0.03462958 0.03545635 0.02994142 0.03668936\n",
      " 0.03688485 0.03891077 0.03999445 0.02899861 0.04301855 0.04701859\n",
      " 0.03073032 0.02840957 0.03095834 0.03758223 0.0382137  0.03874763\n",
      " 0.0378309  0.03969611 0.03715937 0.03395271 0.03565038 0.03312064\n",
      " 0.0278115  0.02818209 0.03586308 0.03555059]\n",
      "             0         2         3         5         6         10        19  \\\n",
      "0      1.000000  0.111963  0.663594  0.815385  0.095238  0.582024  0.000000   \n",
      "1      1.000000  0.130368  0.604743  0.791946  0.071429  0.611043  0.000000   \n",
      "2      1.000000  0.107873  0.575130  0.663866  0.071429  0.546334  0.000000   \n",
      "3      1.000000  0.271472  0.503788  0.665635  0.214286  0.547768  0.000000   \n",
      "4      1.000000  0.189162  0.559889  0.698198  0.047619  0.542118  0.000000   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "23941  0.897649  0.589904  0.443164  0.623537  0.386775  0.548773  0.055642   \n",
      "23942  0.654926  0.252681  0.538616  0.721910  0.123388  0.555798  0.042678   \n",
      "23943  0.770648  0.086904  0.669819  0.795613  0.169425  0.569926  0.096961   \n",
      "23944  0.033955  0.000000  0.000000  0.000000  0.000000  0.000000  0.060847   \n",
      "23945  0.338383  0.218635  0.578812  0.662140  0.710039  0.616514  0.068705   \n",
      "\n",
      "             20        23        24  ...        41        42        43  \\\n",
      "0      0.002185  0.000000  0.000002  ...  0.025437  0.024138  0.521617   \n",
      "1      0.002185  0.000000  0.000002  ...  0.035177  0.035005  0.341246   \n",
      "2      0.002185  0.000000  0.000002  ...  0.016698  0.730530  0.702222   \n",
      "3      0.002185  0.000000  0.000002  ...  0.011817  0.011429  0.429850   \n",
      "4      0.002185  0.000000  0.000002  ...  0.004455  0.669352  0.437409   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "23941  0.003260  0.005518  0.004428  ...  0.249293  0.323602  0.514313   \n",
      "23942  0.002599  0.003118  0.000002  ...  0.002007  0.319975  0.528768   \n",
      "23943  0.003641  0.005061  0.003776  ...  0.193861  0.008571  0.592665   \n",
      "23944  0.002722  0.012882  0.003623  ...  0.903997  0.011429  0.000000   \n",
      "23945  0.002880  0.006117  0.002321  ...  0.585907  0.002124  0.601959   \n",
      "\n",
      "             44        45        46        47        48        49        52  \n",
      "0      0.833774  0.334855  0.084540  0.769231  0.230769  0.003029  0.999600  \n",
      "1      0.834043  0.316340  0.096807  0.733333  0.266667  0.002295  0.999864  \n",
      "2      0.834873  0.417062  0.058497  0.857143  0.142857  0.003967  0.999467  \n",
      "3      0.833813  0.303829  0.127845  0.666667  0.333333  0.003088  0.999577  \n",
      "4      0.833672  0.218018  0.166795  0.523810  0.476190  0.002805  0.777143  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "23941  0.833778  0.439460  0.215797  0.632974  0.367026  0.003295  0.999520  \n",
      "23942  0.834023  0.328203  0.161954  0.630262  0.369738  0.003191  0.999754  \n",
      "23943  0.833673  0.354187  0.086899  0.774559  0.225441  0.001868  0.999625  \n",
      "23944  0.833333  0.000000  0.000000  0.000000  0.000000  0.000000  1.000000  \n",
      "23945  0.834293  0.302119  0.066473  0.797722  0.202278  0.003349  0.999620  \n",
      "\n",
      "[23946 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "lista =[]\n",
    "for importance, feature in zip(model.feature_importances_, X.columns):\n",
    "    if importance < 0.02:\n",
    "        lista.append(feature)\n",
    "\n",
    "#print(lista)\n",
    "\n",
    "    \n",
    "#New Dataset\n",
    "df2 = X.drop(X.columns[lista],axis = 1)\n",
    "df2.head()\n",
    "# compute feature_importances again\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(df2, y.ravel())\n",
    "importance = rf.feature_importances_\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "print(importance)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  2,  3,  5,  6, 10, 19, 20, 23, 24, 25, 26, 27, 28, 29, 38, 39,\n",
       "            40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 52],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your comments for step 2.4 Feature importance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Model Selection (up to 8.2 of 11.2  points)\n",
    "In this part of the challenge you are requested to perform all the necessary steps required in order to design a full fledged classification task on the <b>shares</b> feature.\n",
    "\n",
    "You are requested to perform the following steps having in mind the following: \n",
    "\n",
    "1) the dataset must be properly splitted to perform crossvalidation \n",
    "\n",
    "2) when required, features must be properly encoded\n",
    "\n",
    "3) in order to simplify the problem the target feature can be dicretized <b>(number of classes must be >=5)</b> ;\n",
    "\n",
    "4) for model selection you are requested to consider: \n",
    "\n",
    "- Decision Trees\n",
    "\n",
    "- Support Vector Machines;\n",
    "\n",
    "- An ensamble methodology;\n",
    "\n",
    "- MLPNs.\n",
    "\n",
    "5) hyper-parameter tuning <b>must</b> be performed and discussed;\n",
    "\n",
    "6) apply standardizion and normalization when appropriate;\n",
    "\n",
    "7) remember to use an appropriate evaluation setting (cross-fold etc..)\n",
    "\n",
    "8) describe the measures adopted for the evaluation and discuss the results;\n",
    "\n",
    "9) provide a discussion of the model selection, where you describe the differences in terms of performance and explains the root causes;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALING \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer \n",
    "X = df2\n",
    "#scaler = MinMaxScaler()\n",
    "#print(X.head())\n",
    "#x_scal = scaler.fit_transform(X)\n",
    "\n",
    "#X.head()\n",
    "#print(x_scal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.21201563, 0.23001647, 0.20401573, 0.22901487, 0.21701241,\n",
       "        0.2140131 , 0.22201729, 0.20301437, 0.19201183, 0.21901774]),\n",
       " 'score_time': array([0.00400019, 0.00500035, 0.00400066, 0.00600028, 0.00400114,\n",
       "        0.00400114, 0.00399995, 0.00400114, 0.00499988, 0.00499964]),\n",
       " 'test_accuracy': array([0.11816284, 0.23382046, 0.22129436, 0.21294363, 0.23549061,\n",
       "        0.22254697, 0.24644946, 0.23099415, 0.06056809, 0.22681704]),\n",
       " 'test_f1_micro': array([0.11816284, 0.23382046, 0.22129436, 0.21294363, 0.23549061,\n",
       "        0.22254697, 0.24644946, 0.23099415, 0.06056809, 0.22681704]),\n",
       " 'test_neg_mean_squared_error': array([-4.0308977 , -6.48434238, -6.62004175, -6.15824635, -6.14780793,\n",
       "        -4.36951983, -5.13617377, -5.0112782 , -4.73350042, -5.55263158])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Trees:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    " \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#clf = DecisionTreeClassifier(random_state=0, criterion= \"gini\",min_samples_split =2 )\n",
    "clf_model = DecisionTreeClassifier(criterion=\"gini\", random_state=42,max_depth=3, min_samples_leaf=5)   \n",
    "clf_model.fit(X_train,y_train)\n",
    "#clf.fit(X_train, y_train)\n",
    "y_pred_tree = clf_model.predict(X_test)\n",
    "y_tree = y_test\n",
    "#sklearn.model_selection.cross_validate()\n",
    "cross_validate(clf_model, X, y,scoring=('accuracy','f1_micro',\"neg_mean_squared_error\"),cv=10)\n",
    "#cross_val_score(clf_model, X, y,scoring=('neg_root_mean_squared_error','accuracy','recall'), cv=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted', 'matthews_corrcoef', 'max_error', 'mutual_info_score', 'neg_brier_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error', 'neg_mean_gamma_deviance', 'neg_mean_poisson_deviance', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'neg_root_mean_squared_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'rand_score', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'roc_auc_ovo', 'roc_auc_ovo_weighted', 'roc_auc_ovr', 'roc_auc_ovr_weighted', 'top_k_accuracy', 'v_measure_score']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics \n",
    "keys = sorted(metrics.SCORERS.keys())\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "I decided to tune the following hyperparameters:\n",
    "**Min sample split**\n",
    "he minimum number of samples required to split an internal node\n",
    "**Max Leaf Node** \n",
    "Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_leaf_nodes=13, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_leaf_nodes=13, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_leaf_nodes=13, random_state=42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'max_leaf_nodes': list(range(2, 20)), 'min_samples_split': [2, 3, 4, 5]}\n",
    "grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)\n",
    "grid_search_cv.fit(X_train, y_train)\n",
    "grid_search_cv.best_estimator_\n",
    "#https://medium.com/analytics-vidhya/decisiontree-classifier-working-on-moons-dataset-using-gridsearchcv-to-find-best-hyperparameters-ede24a06b489#:~:text=Save-,DecisionTree%20Classifier%20%E2%80%94%20Working%20on%20Moons%20Dataset%20using%20GridSearchCV%20to%20find,logic%20behind%20decision%20tree's%20classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv_predictions = grid_search_cv.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.40      0.32      1214\n",
      "         1.0       0.27      0.18      0.22      1211\n",
      "         2.0       0.21      0.15      0.17      1181\n",
      "         3.0       0.21      0.04      0.06      1220\n",
      "         4.0       0.22      0.26      0.24      1193\n",
      "         5.0       0.28      0.50      0.36      1165\n",
      "\n",
      "    accuracy                           0.25      7184\n",
      "   macro avg       0.24      0.26      0.23      7184\n",
      "weighted avg       0.24      0.25      0.23      7184\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25389755011135856"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, grid_cv_predictions))\n",
    "\n",
    "grid_search_cv.best_estimator_.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([ 0,  2,  3,  5,  6, 10, 19, 20, 23, 24, 25, 26, 27, 28, 29, 38, 39,\n",
      "            40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 52],\n",
      "           dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([14.83583522, 15.18744302]),\n",
       " 'score_time': array([21.98643899, 20.59058118]),\n",
       " 'test_accuracy': array([0.22032907, 0.18057296]),\n",
       " 'test_max_error': array([-5., -5.]),\n",
       " 'test_neg_mean_squared_error': array([-5.17464295, -3.34118433])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#support vector machines\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "sup = SVC()\n",
    "sup.fit(X_train, y_train.ravel())\n",
    "y_pred_svm0 =  sup.predict(X_test)\n",
    "y_true_0 = y_test\n",
    "#sup_val = cross_val_score(sup, X, y, scoring='accuracy', cv=10)\n",
    "cross_validate(sup, X, y,scoring=('accuracy','max_error',\"neg_mean_squared_error\"),cv=2)\n",
    "#sup_val = cross_val_score(sup, X, y, scoring='rmse', cv=10)\n",
    "#print(sup_val ,\"AVARAGE = \", sup_val.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter Tuning\n",
    "I decided to tune the following hyperparameter: \n",
    "**C** : Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "\n",
    "**gamma** \n",
    "\n",
    "**Kernel** Specifies the kernel type to be used in the algorithm. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.239 total time=  33.8s\n",
      "[CV 2/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.238 total time=  32.6s\n",
      "[CV 3/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.231 total time=  32.4s\n",
      "[CV 4/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.246 total time=  32.7s\n",
      "[CV 5/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.242 total time=  32.7s\n",
      "[CV 1/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.198 total time=  34.1s\n",
      "[CV 2/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.199 total time=  34.0s\n",
      "[CV 3/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.199 total time=  37.9s\n",
      "[CV 4/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.198 total time=  36.0s\n",
      "[CV 5/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.200 total time=  34.9s\n",
      "[CV 1/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.168 total time=  35.2s\n",
      "[CV 2/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.168 total time=  34.1s\n",
      "[CV 3/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.168 total time=  34.9s\n",
      "[CV 4/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.168 total time=  33.5s\n",
      "[CV 5/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.168 total time=  34.1s\n",
      "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.250 total time=  31.5s\n",
      "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.251 total time=  31.1s\n",
      "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.246 total time=  31.2s\n",
      "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.255 total time=  30.8s\n",
      "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.245 total time=  31.2s\n",
      "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.238 total time=  32.8s\n",
      "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.238 total time=  36.3s\n",
      "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.233 total time=  32.5s\n",
      "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.248 total time=  33.0s\n",
      "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.242 total time=  33.0s\n",
      "[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.198 total time=  34.2s\n",
      "[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.199 total time=  34.5s\n",
      "[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.198 total time=  33.6s\n",
      "[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.197 total time=  37.1s\n",
      "[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.203 total time=  34.2s\n",
      "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.257 total time=  31.5s\n",
      "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.254 total time=  31.5s\n",
      "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.249 total time=  31.3s\n",
      "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.259 total time=  31.5s\n",
      "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.248 total time=  31.4s\n",
      "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.245 total time=  31.3s\n",
      "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.246 total time=  31.2s\n",
      "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.241 total time=  31.1s\n",
      "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.255 total time=  30.8s\n",
      "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.239 total time=  31.2s\n",
      "[CV 1/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.238 total time=  31.9s\n",
      "[CV 2/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.238 total time=  32.0s\n",
      "[CV 3/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.233 total time=  32.2s\n",
      "[CV 4/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.247 total time=  32.2s\n",
      "[CV 5/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.242 total time=  31.9s\n",
      "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "SVC(C=10, gamma=0.1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    " \n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1,10],\n",
    "              'gamma': [0.1, 0.01,0.001 ],\n",
    "              'kernel': ['rbf']}\n",
    " \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    " \n",
    "# fitting the model for grid search\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "grid_predictions = grid.best_estimator_.predict(X_test)\n",
    "\n",
    "# print best parameter after tuning\n",
    "print(grid.best_params_)\n",
    " \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)\n",
    " \n",
    "# print classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.44      0.33       803\n",
      "         1.0       0.27      0.22      0.24       846\n",
      "         2.0       0.21      0.07      0.10       790\n",
      "         3.0       0.23      0.15      0.18       781\n",
      "         4.0       0.23      0.33      0.27       776\n",
      "         5.0       0.32      0.37      0.34       794\n",
      "\n",
      "    accuracy                           0.26      4790\n",
      "   macro avg       0.25      0.26      0.24      4790\n",
      "weighted avg       0.25      0.26      0.24      4790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Print Best Estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2615866388308977"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: (0.2199949887246304, 'Logistic Regression')\n",
      "Accuracy: (0.2390795957571202, 'Random Forest')\n",
      "Accuracy: (0.19360227177816755, 'naive Bayes')\n",
      "Accuracy: (0.20721623653219745, 'Ensamble')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "#Initialize Logistic regression, Random forest, Gaussian Naive Bayes classifiers \n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "rndf = RandomForestClassifier()\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#initialize Ensamble voting classifier\n",
    "\n",
    "ensamble = VotingClassifier(\n",
    "estimators=[('lr', logreg), ('rf', rndf), ('gnb', gnb)], voting='hard')\n",
    "ensamble.fit(X_train, y_train.ravel())\n",
    "\n",
    "#display and avarage results\n",
    "\n",
    "for clf, label in zip([logreg, rndf, gnb, ensamble], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensamble']):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=3)\n",
    "    print(\"Accuracy:\" ,(scores.mean(), label))\n",
    "y_pred_ens = ensamble.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr__C': 100.0, 'rf__n_estimators': 200, 'voting': 'hard'}\n",
      "VotingClassifier(estimators=[('lr',\n",
      "                              LogisticRegression(C=100.0, max_iter=10000,\n",
      "                                                 random_state=1)),\n",
      "                             ('rf',\n",
      "                              RandomForestClassifier(n_estimators=200,\n",
      "                                                     random_state=1)),\n",
      "                             ('gnb', GaussianNB())])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.24      0.69      0.36       803\n",
      "         1.0       0.31      0.11      0.16       846\n",
      "         2.0       0.20      0.20      0.20       790\n",
      "         3.0       0.26      0.06      0.10       781\n",
      "         4.0       0.31      0.17      0.22       776\n",
      "         5.0       0.35      0.35      0.35       794\n",
      "\n",
      "    accuracy                           0.26      4790\n",
      "   macro avg       0.28      0.26      0.23      4790\n",
      "weighted avg       0.28      0.26      0.23      4790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "clf1 = LogisticRegression(random_state=1, max_iter=10000)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],voting='soft')\n",
    "\n",
    "params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200],\"voting\":[\"hard\",\"soft\"]}\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "grid = grid.fit(X_train, y_train)\n",
    "grid_predictions = grid.best_estimator_.predict(X_test)\n",
    "\n",
    "print(grid.best_params_)\n",
    " \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)\n",
    " \n",
    "# print classification report\n",
    "print(classification_report(y_test, grid_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2640918580375783"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2582463465553236"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(max_iter=500)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.predict(X_test)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Hyperparameter tuning\n",
    "\n",
    "I tuned the followinjg Hyperparamters : \n",
    "**Activations**: Activation function for the hidden layer.\n",
    "\n",
    "**Hidden_layer_size**:The ith element represents the number of neurons in the ith hidden layer.\n",
    "\n",
    "**solver** :The solver for weight optimization.\n",
    "\n",
    "\n",
    "\"sgd\" refers to stochastic gradient descent.\n",
    "\n",
    "\"adam\" refers to a stochastic gradient-based optimizer \n",
    "\n",
    "**alpha**:Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss.\n",
    "\n",
    "**Learning_rate**: Learning rate schedule for weight updates.\n",
    "\n",
    "\"constant\" is a constant learning rate given by ‘learning_rate_init’.\n",
    "\n",
    "\"adaptive\" keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'tanh', 'alpha': 0.05, 'hidden_layer_sizes': (10, 30, 10), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "MLPClassifier(activation='tanh', alpha=0.05, hidden_layer_sizes=(10, 30, 10),\n",
      "              max_iter=500)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.52      0.36       803\n",
      "         1.0       0.24      0.14      0.18       846\n",
      "         2.0       0.18      0.09      0.12       790\n",
      "         3.0       0.20      0.08      0.11       781\n",
      "         4.0       0.26      0.29      0.28       776\n",
      "         5.0       0.31      0.46      0.37       794\n",
      "\n",
      "    accuracy                           0.26      4790\n",
      "   macro avg       0.24      0.26      0.24      4790\n",
      "weighted avg       0.24      0.26      0.24      4790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "    'activation': ['tanh', 'relu',\"identity\", \"logistic\" ],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],}\n",
    "\n",
    "grid = GridSearchCV(estimator=clf, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid = grid.fit(X_train, y_train)\n",
    "grid_predictions = grid.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "print(grid.best_params_)\n",
    " \n",
    "# print how our model looks after hyper-parameter tuning\n",
    "print(grid.best_estimator_)\n",
    " \n",
    "# print classification report\n",
    "print(classification_report(y_test, grid_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2640918580375783"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write here your comments for step 3 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# 4. Summary\n",
    "Provide a summary discussion (in English) of your solution <b>(at least 500 words)</b> feel free to include plots figures and tables.\n",
    "\n",
    "<b>This is a mandatory step</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write here <b>your own</b> summary dicussion (in English) use at least 500 words, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part I tried plotting some statistics about the dataset as the number of null/none values, general statistics using \"data.describe\"  that gives us min, max,std count and many others for each feature.\n",
    "The null values were very few. It could be easily deleted but I opted to replace them.\n",
    "I decided to replace all none and null values using Simple Imputer with the strategy mean.\n",
    "In order to increase accuracy of each classifier I decided to scale the dataset between 0,1 with MinMax classifier.\n",
    "\n",
    "In the second Part after plotting the numbers of columns, the first 15 columns of table, the distribution of share and the A bar chart counting the attributes required I decide to discretize with \"K-bins Discretizer '' the share column in 6 bins using quantile strategy. \n",
    "Discretize means divide the continuous values of share in 6 different classes called bins. \n",
    "I noticed in the plot that bins are unbalanced so I decided to resample it.\n",
    "I used the SMOTE algorithm technique, which creates new, artificial samples by tracing lines between pre-existing samples in their feature space and choosing points in these lines to be added to the dataset. As you can see in the relative plot, after SMOTE our classes are perfectly balanced.\n",
    "After I decide to find the most relevant features in data in order to construct a solid and accurate model.\n",
    "I decided to use a \"Random Forest\" model and \"feature_importances\" that gives us the features most relevant to predict the y. I decided to create a new dataset with the only selected features.\n",
    "Finally I plot the result of feature importance analysis highlighting that I found about 25 relevant features from the 60 originally.\n",
    "\n",
    "\n",
    "\n",
    "In the third part I create all the models required to predict “share” columns.I decide to make crossValidation for each model and use GridParams to tune Hyperparameters.\n",
    "For the evaluation phase I decided to use a Cross Validation and classification report that gives the results in terms of accuracy, f1 score, precision and recall. \n",
    "The F1-score combines the precision and recall of a classifier into a single metric by taking their harmonic mean. I decided to use it in order to compare the performance of classifiers.\n",
    "\n",
    "\n",
    "I started with Decision Tree, I applied cross validation with 10 folds and hyper parameters tuning with the following parameters to be tuned \"('max_leaf_nodes': list(range(2, 20)), 'min_samples_split': [2, 3, 4, 5])\"\n",
    "The best parameters fund is \"(max_leaf_nodes=15, random_state=42\"\n",
    "The results show an accuracy around 22%.\n",
    "Decision Tree is one of the oldest ML models and its accuracy is low with respect new and more complicated models like Neural Network.\n",
    "\n",
    "\n",
    "After I create a SVM model, I applied cross validation with 2 folds and hyper parameter tuning with the following parameters to be tuned \" 'C': [0.1, 1],'gamma': [0.1, 0.01 ],'kernel': ['rbf']\" \"\n",
    "The best parameters fund is \"(max_leaf_nodes=15, random_state=42\"\n",
    "The results shows an accuracy around the \"25%\"\n",
    "\n",
    "\n",
    "I create  models using Ensemble methods combining LogisticRegression, GaussianNaiveBayes ,RandomForestClassifier, VotingClassifier.\n",
    "I applied cross validation with 3 folds and I tuned the following Hyperparameter : \" 'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200] \"\n",
    "The best parameters found are \"'lr__C': 100.0, 'rf__n_estimators': 20\".\n",
    "The results show an accuracy around the \"20%\".\n",
    "\n",
    "\n",
    "Finally the last model created is Multilayer Perceptron Network.\n",
    "As usually i tuned some hyperparameters : \"epochs=[10,20,30]\"\n",
    "The best parameters found is \"MLPClassifier(activation='tanh', hidden_layer_sizes=(10, 30, 10),random_state=1)\"\n",
    "The results shows an accuracy around the \"30%\"\n",
    "In general all the models trained perform very similarly with an accuracy of 30%.\n",
    "MLP seems to perform better because NN are generally faster but at the same time complex respect other methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "36fabd57e396b83aef36b02305bae0bcb053b13a3fa699e04cd3b884d5079903"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
